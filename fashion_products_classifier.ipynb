{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeMdas3xE_z_"
      },
      "source": [
        "Installing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgrQtiuwDQge",
        "outputId": "019feea3-cbee-48bd-e6c7-39edff079635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "#move kaggle.json into files first\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZD-MxeYAoct",
        "outputId": "a5f3b5f9-81ad-421d-e04b-ff14352cb5e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading fashion-product-images-dataset.zip to .\n",
            "100% 23.1G/23.1G [07:21<00:00, 90.6MB/s]\n",
            "100% 23.1G/23.1G [07:21<00:00, 56.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download paramaggarwal/fashion-product-images-dataset -p ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iPIVjYkzELUz"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/fashion-product-images-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-fOy_3_mH28m"
      },
      "outputs": [],
      "source": [
        "!rm /content/fashion-product-images-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPOlu6aGqL0I"
      },
      "source": [
        "CSV file processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MYK8RTdRqQji",
        "outputId": "9e378483-cfde-4a6f-e9e7-d6149da00ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The classes are:  {'Accessories': 0, 'Apparel Set': 1, 'Bags': 2, 'Bath and Body': 3, 'Beauty Accessories': 4, 'Belts': 5, 'Bottomwear': 6, 'Cufflinks': 7, 'Dress': 8, 'Eyes': 9, 'Eyewear': 10, 'Flip Flops': 11, 'Fragrance': 12, 'Free Gifts': 13, 'Gloves': 14, 'Hair': 15, 'Headwear': 16, 'Home Furnishing': 17, 'Innerwear': 18, 'Jewellery': 19, 'Lips': 20, 'Loungewear and Nightwear': 21, 'Makeup': 22, 'Mufflers': 23, 'Nails': 24, 'Perfumes': 25, 'Sandal': 26, 'Saree': 27, 'Scarves': 28, 'Shoe Accessories': 29, 'Shoes': 30, 'Skin': 31, 'Skin Care': 32, 'Socks': 33, 'Sports Accessories': 34, 'Sports Equipment': 35, 'Stoles': 36, 'Ties': 37, 'Topwear': 38, 'Umbrellas': 39, 'Vouchers': 40, 'Wallets': 41, 'Watches': 42, 'Water Bottle': 43, 'Wristbands': 44}\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "image_names = []\n",
        "class_names = []\n",
        "\n",
        "f1 = open(\"/content/fashion-dataset/styles.csv\")\n",
        "csv_f1 = csv.reader(f1)\n",
        "for i, row in enumerate(csv_f1):\n",
        "  if i != 0:\n",
        "    image_names.append(str(row[0]))\n",
        "    class_names.append(str(row[3]))\n",
        "f1.close()\n",
        "\n",
        "unique_classes = np.unique(class_names)\n",
        "class_dict = dict()\n",
        "for i in range(len(unique_classes)):\n",
        "  class_dict[unique_classes[i]] = i\n",
        "print(\"The classes are: \", class_dict)\n",
        "\n",
        "##Some files were found to be missing in this Kaggle dataset\n",
        "##These will be skipped when writing the CSV file\n",
        "missing_files = {'39403', '39410', '39401', '39425', '12347'}\n",
        "\n",
        "f2 = open(\"/content/fashion-dataset/labels.csv\", mode='w', newline='')\n",
        "csv_f2 = csv.writer(f2, delimiter=',')\n",
        "csv_f2.writerow(['filename', 'class'])\n",
        "for i in range(len(image_names)):\n",
        "  if image_names[i] not in missing_files:\n",
        "    csv_f2.writerow([image_names[i]+'.jpg', class_dict[class_names[i]]])\n",
        "f2.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8YXSkX4dFUu"
      },
      "source": [
        "Importing libraries + setting seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fUefwWSSbk0n",
        "outputId": "87a00e5d-9c37-4c9b-d9c1-0340dd13155c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7feb33e1a9d0>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "#from skimage import io\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChxUG2fFdRg7"
      },
      "source": [
        "Image transformations, datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hyA83Y-vdDP2",
        "outputId": "3587efa1-c57a-499b-9e73-f0cf631dc379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training images:  35000 ; validation images: 4441 ; test images:  5000\n"
          ]
        }
      ],
      "source": [
        "#https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "img_dir = \"/content/fashion-dataset/fashion-dataset/images\"\n",
        "batch_size = 64 #128 #512 #256 #revisit this\n",
        "\n",
        "#transformations\n",
        "def normalize_transform():\n",
        "  return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "                                      transforms.Resize(256), \n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      normalize_transform()\n",
        "                                      ])\n",
        "\n",
        "#custom datasets\n",
        "class FashionProducts(torch.utils.data.Dataset):\n",
        "  def  __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "    self.img_labels = pd.read_csv(annotations_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    #print(img_path)\n",
        "    image = Image.open(img_path)\n",
        "    #image = io.imread(img_path)\n",
        "    #image = image.to(dtype=torch.float32)\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    return image, label\n",
        "\n",
        "trainvaltest_dataset = FashionProducts(annotations_file = \"/content/fashion-dataset/labels.csv\",\n",
        "                                img_dir = img_dir,\n",
        "                                transform=data_transforms\n",
        "                                )\n",
        "\n",
        "#splitting dataset\n",
        "train_size = 35000\n",
        "test_size = 5000\n",
        "val_size = len(trainvaltest_dataset) - train_size - test_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(trainvaltest_dataset, [train_size, val_size, test_size],\n",
        "                                                                         generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "#dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(seed))\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"training images: \",len(train_dataset), \"; validation images:\", len(val_dataset), \"; test images: \", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eWS1BWZf-Ul"
      },
      "source": [
        "DON'T RUN FURTHER TRAINING/VALIDATION CELLS UNLESS YOU KNOW WHAT YOU'RE DOING. RISK OF OVERWRITING MODEL WEIGHTS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V8ny1PvnCv5"
      },
      "source": [
        "Setting hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqYb64g0nDPC"
      },
      "outputs": [],
      "source": [
        "#from torchvision.ops.misc import ConvNormActivation\n",
        "#https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#finetuning-the-convnet\n",
        "num_classes = 45\n",
        "num_epochs = 2\n",
        "learning_rate = 0.05 #0.000025 #0.256\n",
        "##weight_decay = 0.9\n",
        "momentum = 0.9\n",
        "\n",
        "#importing pretrained model, freezing layers, and changing output layer\n",
        "###model = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "#for param in model.parameters():\n",
        "#  param.requires_grad = False\n",
        "\n",
        "#lastconv_input_channels = model.lastconv_input_channels\n",
        "#lastconv_output_channels = model.lastconv_output_channels\n",
        "#norm_layer = model.norm_layer\n",
        "#\n",
        "#model.features[-1] = ConvNormActivation(\n",
        "#                lastconv_input_channels,\n",
        "#                lastconv_output_channels,\n",
        "#                kernel_size=1,\n",
        "#                norm_layer=norm_layer,\n",
        "#                activation_layer=nn.SiLU,\n",
        "#            )\n",
        "\n",
        "###num_features = model.classifier[1].in_features #https://pytorch.org/vision/stable/_modules/torchvision/models/efficientnet.html#efficientnet_b0\n",
        "###model.classifier[1] = nn.Linear(num_features, num_classes) #replaces last layer\n",
        "###nn.init.kaiming_normal_(model.classifier[1].weight) #initializes weights of last layer\n",
        "\n",
        "##model = torchvision.models.resnet50(pretrained=True)\n",
        "model = torchvision.models.resnet152(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "#check params in original paper\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
        "\n",
        "###optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=weight_decay, momentum=momentum, centered=False)\n",
        "###scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1500], gamma=0.1)\n",
        "#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i for i in range(1, num_epochs)], gamma=0.97) #original paper uses 2.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OutZ2bxpnDsV"
      },
      "source": [
        "Begin training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6ehDOHj8lpj",
        "outputId": "d6f8076a-041a-4202-e1ad-732cbbee2aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Are you sure you want to begin training and saving models? Select (Y/N): Y\n",
            "Batch loss: 3.8393478393554688\n",
            "Batch loss: 4.127566814422607\n",
            "Batch loss: 3.9593968391418457\n",
            "Batch loss: 2.37223482131958\n",
            "Batch loss: 1.9328553676605225\n",
            "Batch loss: 0.9573944211006165\n",
            "Batch loss: 1.9604616165161133\n",
            "Batch loss: 1.4972474575042725\n",
            "Batch loss: 0.8654681444168091\n",
            "Batch loss: 1.5320483446121216\n",
            "Batch loss: 1.5422035455703735\n",
            "Batch loss: 1.0336294174194336\n",
            "Batch loss: 1.0199122428894043\n",
            "Batch loss: 0.7696935534477234\n",
            "Batch loss: 1.0358219146728516\n",
            "Batch loss: 0.9249795079231262\n",
            "Batch loss: 0.8556882739067078\n",
            "Batch loss: 1.250800609588623\n",
            "Batch loss: 0.8107422590255737\n",
            "Batch loss: 1.5717309713363647\n",
            "Batch loss: 1.5552647113800049\n",
            "Batch loss: 1.074620246887207\n",
            "Batch loss: 1.2444936037063599\n",
            "Batch loss: 2.721358060836792\n",
            "Batch loss: 1.2774097919464111\n",
            "Batch loss: 1.5310537815093994\n",
            "Batch loss: 2.0231130123138428\n",
            "Batch loss: 2.0269765853881836\n",
            "Batch loss: 1.6906955242156982\n",
            "Batch loss: 1.4342987537384033\n",
            "Batch loss: 1.4284253120422363\n",
            "Batch loss: 1.5834338665008545\n",
            "Batch loss: 2.3299689292907715\n",
            "Batch loss: 1.5030617713928223\n",
            "Batch loss: 1.7192212343215942\n",
            "Batch loss: 1.1495198011398315\n",
            "Batch loss: 1.1983476877212524\n",
            "Batch loss: 1.349785566329956\n",
            "Batch loss: 1.4148656129837036\n",
            "Batch loss: 1.5410785675048828\n",
            "Batch loss: 1.4238781929016113\n",
            "Batch loss: 1.1640833616256714\n",
            "Batch loss: 1.3531259298324585\n",
            "Batch loss: 1.416577696800232\n",
            "Batch loss: 1.2367839813232422\n",
            "Batch loss: 1.3278878927230835\n",
            "Batch loss: 0.7602810263633728\n",
            "Batch loss: 1.1353954076766968\n",
            "Batch loss: 1.2286219596862793\n",
            "Batch loss: 1.8803110122680664\n",
            "Batch loss: 0.9928816556930542\n",
            "Batch loss: 1.6232601404190063\n",
            "Batch loss: 1.1493268013000488\n",
            "Batch loss: 1.2675708532333374\n",
            "Batch loss: 1.0520508289337158\n",
            "Batch loss: 1.2144466638565063\n",
            "Batch loss: 1.1426427364349365\n",
            "Batch loss: 0.9873005151748657\n",
            "Batch loss: 0.8944830894470215\n",
            "Batch loss: 0.6402503252029419\n",
            "Batch loss: 1.5939618349075317\n",
            "Batch loss: 0.9540286064147949\n",
            "Batch loss: 0.7632372975349426\n",
            "Batch loss: 0.7912567853927612\n",
            "Batch loss: 0.8380129933357239\n",
            "Batch loss: 1.3257712125778198\n",
            "Batch loss: 0.9971159100532532\n",
            "Batch loss: 0.822820246219635\n",
            "Batch loss: 0.9404058456420898\n",
            "Batch loss: 0.9186506271362305\n",
            "Batch loss: 0.8100785613059998\n",
            "Batch loss: 0.9660846590995789\n",
            "Batch loss: 0.758385956287384\n",
            "Batch loss: 0.9853183031082153\n",
            "Batch loss: 1.072192907333374\n",
            "Batch loss: 1.0824460983276367\n",
            "Batch loss: 0.7808995842933655\n",
            "Batch loss: 1.0847809314727783\n",
            "Batch loss: 0.6842173933982849\n",
            "Batch loss: 0.8489582538604736\n",
            "Batch loss: 0.8020015954971313\n",
            "Batch loss: 1.0533065795898438\n",
            "Batch loss: 1.1700060367584229\n",
            "Batch loss: 1.175194501876831\n",
            "Batch loss: 0.8913090229034424\n",
            "Batch loss: 1.1465530395507812\n",
            "Batch loss: 0.6683640480041504\n",
            "Batch loss: 0.7009987831115723\n",
            "Batch loss: 0.8096125721931458\n",
            "Batch loss: 0.5572513937950134\n",
            "Batch loss: 0.5371715426445007\n",
            "Batch loss: 0.5827765464782715\n",
            "Batch loss: 0.6443201899528503\n",
            "Batch loss: 0.7224494218826294\n",
            "Batch loss: 0.6862883567810059\n",
            "Batch loss: 0.46578076481819153\n",
            "Batch loss: 0.7853162884712219\n",
            "Batch loss: 0.4322971701622009\n",
            "Batch loss: 1.006091594696045\n",
            "Batch loss: 0.7690721750259399\n",
            "Batch loss: 1.1033680438995361\n",
            "Batch loss: 0.7840875387191772\n",
            "Batch loss: 0.6752015352249146\n",
            "Batch loss: 0.8802641034126282\n",
            "Batch loss: 0.8434203863143921\n",
            "Batch loss: 0.36907270550727844\n",
            "Batch loss: 0.5850647687911987\n",
            "Batch loss: 0.6841709017753601\n",
            "Batch loss: 0.9136344194412231\n",
            "Batch loss: 0.7422478795051575\n",
            "Batch loss: 0.8792123794555664\n",
            "Batch loss: 0.4950141906738281\n",
            "Batch loss: 1.259222149848938\n",
            "Batch loss: 0.4944037199020386\n",
            "Batch loss: 0.3512607216835022\n",
            "Batch loss: 0.8756487369537354\n",
            "Batch loss: 0.8295738101005554\n",
            "Batch loss: 1.0357199907302856\n",
            "Batch loss: 0.8875516653060913\n",
            "Batch loss: 1.1952048540115356\n",
            "Batch loss: 1.097617506980896\n",
            "Batch loss: 0.8374882340431213\n",
            "Batch loss: 0.8991210460662842\n",
            "Batch loss: 0.6994877457618713\n",
            "Batch loss: 0.6087385416030884\n",
            "Batch loss: 0.40586817264556885\n",
            "Batch loss: 0.6964874267578125\n",
            "Batch loss: 0.811129093170166\n",
            "Batch loss: 0.7122138142585754\n",
            "Batch loss: 0.8642236590385437\n",
            "Batch loss: 0.9609823226928711\n",
            "Batch loss: 0.6287598609924316\n",
            "Batch loss: 0.5789856910705566\n",
            "Batch loss: 0.5102611780166626\n",
            "Batch loss: 0.6961419582366943\n",
            "Batch loss: 0.7411791086196899\n",
            "Batch loss: 0.5591866970062256\n",
            "Batch loss: 0.4824376106262207\n",
            "Batch loss: 0.5798103213310242\n",
            "Batch loss: 0.45623427629470825\n",
            "Batch loss: 0.5415806770324707\n",
            "Batch loss: 0.6490424871444702\n",
            "Batch loss: 0.4945886433124542\n",
            "Batch loss: 0.4721188545227051\n",
            "Batch loss: 0.40791550278663635\n",
            "Batch loss: 0.9181663393974304\n",
            "Batch loss: 0.9102025628089905\n",
            "Batch loss: 0.9317638874053955\n",
            "Batch loss: 0.6350893378257751\n",
            "Batch loss: 0.40226221084594727\n",
            "Batch loss: 0.7013905644416809\n",
            "Batch loss: 0.9041292667388916\n",
            "Batch loss: 0.9158840775489807\n",
            "Batch loss: 0.9498875737190247\n",
            "Batch loss: 0.9551324844360352\n",
            "Batch loss: 0.1967330127954483\n",
            "Batch loss: 0.7104322910308838\n",
            "Batch loss: 0.6905667781829834\n",
            "Batch loss: 0.8064892292022705\n",
            "Batch loss: 0.9233294129371643\n",
            "Batch loss: 0.901608407497406\n",
            "Batch loss: 0.909268319606781\n",
            "Batch loss: 0.5871121287345886\n",
            "Batch loss: 0.6248307228088379\n",
            "Batch loss: 0.5567774772644043\n",
            "Batch loss: 0.8527836203575134\n",
            "Batch loss: 0.7296883463859558\n",
            "Batch loss: 0.6216891407966614\n",
            "Batch loss: 0.5429182052612305\n",
            "Batch loss: 0.6281464099884033\n",
            "Batch loss: 0.34977802634239197\n",
            "Batch loss: 0.6692293882369995\n",
            "Batch loss: 0.4473894238471985\n",
            "Batch loss: 0.5580804944038391\n",
            "Batch loss: 0.6979344487190247\n",
            "Batch loss: 0.5682618021965027\n",
            "Batch loss: 0.44859614968299866\n",
            "Batch loss: 0.3987252116203308\n",
            "Batch loss: 0.5098838806152344\n",
            "Batch loss: 0.4760918915271759\n",
            "Batch loss: 0.712042510509491\n",
            "Batch loss: 0.6458737254142761\n",
            "Batch loss: 0.6078845262527466\n",
            "Batch loss: 0.5674559473991394\n",
            "Batch loss: 0.6913894414901733\n",
            "Batch loss: 0.6729229688644409\n",
            "Batch loss: 0.7499523758888245\n",
            "Batch loss: 0.674760639667511\n",
            "Batch loss: 0.8028846979141235\n",
            "Batch loss: 0.5620769262313843\n",
            "Batch loss: 0.8191105127334595\n",
            "Batch loss: 0.5939592719078064\n",
            "Batch loss: 0.39981719851493835\n",
            "Batch loss: 0.2938744127750397\n",
            "Batch loss: 0.452781617641449\n",
            "Batch loss: 0.7015716433525085\n",
            "Batch loss: 0.8039149641990662\n",
            "Batch loss: 0.7299512624740601\n",
            "Batch loss: 0.6326569318771362\n",
            "Batch loss: 0.5836590528488159\n",
            "Batch loss: 1.017082691192627\n",
            "Batch loss: 0.5461567640304565\n",
            "Batch loss: 0.3611987829208374\n",
            "Batch loss: 0.725906252861023\n",
            "Batch loss: 0.4637838304042816\n",
            "Batch loss: 0.4216259717941284\n",
            "Batch loss: 0.4481595456600189\n",
            "Batch loss: 0.5163255333900452\n",
            "Batch loss: 0.4352089762687683\n",
            "Batch loss: 0.42317846417427063\n",
            "Batch loss: 0.7291347980499268\n",
            "Batch loss: 0.5323075652122498\n",
            "Batch loss: 0.6091524958610535\n",
            "Batch loss: 0.45532160997390747\n",
            "Batch loss: 0.4551486670970917\n",
            "Batch loss: 0.5308791399002075\n",
            "Batch loss: 0.6212985515594482\n",
            "Batch loss: 0.48995649814605713\n",
            "Batch loss: 0.5644809603691101\n",
            "Batch loss: 0.48003560304641724\n",
            "Batch loss: 0.4689851999282837\n",
            "Batch loss: 0.4070413112640381\n",
            "Batch loss: 0.4778953194618225\n",
            "Batch loss: 0.5241774916648865\n",
            "Batch loss: 0.47204717993736267\n",
            "Batch loss: 0.38303592801094055\n",
            "Batch loss: 0.3692352771759033\n",
            "Batch loss: 0.7397823929786682\n",
            "Batch loss: 0.7519014477729797\n",
            "Batch loss: 0.5381519198417664\n",
            "Batch loss: 0.5709365010261536\n",
            "Batch loss: 0.3917752802371979\n",
            "Batch loss: 0.7178359627723694\n",
            "Batch loss: 0.8062020540237427\n",
            "Batch loss: 0.5075052976608276\n",
            "Batch loss: 0.7225889563560486\n",
            "Batch loss: 0.7497410774230957\n",
            "Batch loss: 0.36163651943206787\n",
            "Batch loss: 0.40947020053863525\n",
            "Batch loss: 0.44497695565223694\n",
            "Batch loss: 0.6071633100509644\n",
            "Batch loss: 0.5458464622497559\n",
            "Batch loss: 0.29125896096229553\n",
            "Batch loss: 0.5311694741249084\n",
            "Batch loss: 0.5352510213851929\n",
            "Batch loss: 0.4555701017379761\n",
            "Batch loss: 0.7695103287696838\n",
            "Batch loss: 0.7453852295875549\n",
            "Batch loss: 0.7874617576599121\n",
            "Batch loss: 0.19204731285572052\n",
            "Batch loss: 0.11853161454200745\n",
            "Batch loss: 0.725410521030426\n",
            "Batch loss: 0.45630624890327454\n",
            "Batch loss: 0.5625577569007874\n",
            "Batch loss: 0.4245293438434601\n",
            "Batch loss: 0.6156895160675049\n",
            "Batch loss: 0.5206460356712341\n",
            "Batch loss: 0.4885174334049225\n",
            "Batch loss: 0.6159099340438843\n",
            "Batch loss: 0.47910380363464355\n",
            "Batch loss: 0.43337637186050415\n",
            "Batch loss: 0.5774718523025513\n",
            "Batch loss: 0.4034937620162964\n",
            "Batch loss: 0.3951919674873352\n",
            "Batch loss: 0.29112520813941956\n",
            "Batch loss: 0.3656051754951477\n",
            "Batch loss: 0.35783320665359497\n",
            "Batch loss: 0.331459641456604\n",
            "Batch loss: 0.5810750722885132\n",
            "Batch loss: 0.4974266588687897\n",
            "Batch loss: 0.4880905747413635\n",
            "Batch loss: 0.45059311389923096\n",
            "Batch loss: 0.39565932750701904\n",
            "Batch loss: 0.284913569688797\n",
            "Batch loss: 0.5839757919311523\n",
            "Batch loss: 0.7544612288475037\n",
            "Batch loss: 0.7813245058059692\n",
            "Batch loss: 0.3545224666595459\n",
            "Batch loss: 0.43358296155929565\n",
            "Batch loss: 0.35706430673599243\n",
            "Batch loss: 0.6007623076438904\n",
            "Batch loss: 0.4297960102558136\n",
            "Batch loss: 0.2662549614906311\n",
            "Batch loss: 0.44723591208457947\n",
            "Batch loss: 0.39352330565452576\n",
            "Batch loss: 0.3750373125076294\n",
            "Batch loss: 0.4822886884212494\n",
            "Batch loss: 0.23758329451084137\n",
            "Batch loss: 0.5954189896583557\n",
            "Batch loss: 0.5422813892364502\n",
            "Batch loss: 0.5073038339614868\n",
            "Batch loss: 0.3267451822757721\n",
            "Batch loss: 0.5709867477416992\n",
            "Batch loss: 0.2940948009490967\n",
            "Batch loss: 0.5908939242362976\n",
            "Batch loss: 0.3369496166706085\n",
            "Batch loss: 0.5788872838020325\n",
            "Batch loss: 0.34898513555526733\n",
            "Batch loss: 0.23696152865886688\n",
            "Batch loss: 0.6983532309532166\n",
            "Batch loss: 0.43037131428718567\n",
            "Batch loss: 0.6986064314842224\n",
            "Batch loss: 0.3978639841079712\n",
            "Batch loss: 0.38415178656578064\n",
            "Batch loss: 0.39570000767707825\n",
            "Batch loss: 0.25285613536834717\n",
            "Batch loss: 0.4182332456111908\n",
            "Batch loss: 0.4462435245513916\n",
            "Batch loss: 0.5462688207626343\n",
            "Batch loss: 0.39000362157821655\n",
            "Batch loss: 0.4157406687736511\n",
            "Batch loss: 0.3510541617870331\n",
            "Batch loss: 0.41446104645729065\n",
            "Batch loss: 0.44968080520629883\n",
            "Batch loss: 0.4408438503742218\n",
            "Batch loss: 0.5083184838294983\n",
            "Batch loss: 0.2793205976486206\n",
            "Batch loss: 0.3933902978897095\n",
            "Batch loss: 0.262914776802063\n",
            "Batch loss: 0.5335912704467773\n",
            "Batch loss: 0.30640581250190735\n",
            "Batch loss: 0.46647098660469055\n",
            "Batch loss: 0.40722450613975525\n",
            "Batch loss: 0.23386475443840027\n",
            "Batch loss: 0.38212090730667114\n",
            "Batch loss: 0.19419533014297485\n",
            "Batch loss: 0.5030413866043091\n",
            "Batch loss: 0.5167202353477478\n",
            "Batch loss: 0.14049291610717773\n",
            "Batch loss: 0.3743959665298462\n",
            "Batch loss: 0.20418494939804077\n",
            "Batch loss: 0.4960777759552002\n",
            "Batch loss: 0.4665994942188263\n",
            "Batch loss: 0.4816109836101532\n",
            "Batch loss: 0.6657691597938538\n",
            "Batch loss: 0.29065969586372375\n",
            "Batch loss: 0.5178803205490112\n",
            "Batch loss: 0.43359318375587463\n",
            "Batch loss: 0.37695568799972534\n",
            "Batch loss: 0.35723787546157837\n",
            "Batch loss: 0.25351250171661377\n",
            "Batch loss: 0.5799448490142822\n",
            "Batch loss: 1.298979640007019\n",
            "Batch loss: 0.5074433088302612\n",
            "Batch loss: 0.6586520075798035\n",
            "Batch loss: 0.5287837982177734\n",
            "Batch loss: 0.23997487127780914\n",
            "Batch loss: 0.29555559158325195\n",
            "Batch loss: 0.5194651484489441\n",
            "Batch loss: 0.38819023966789246\n",
            "Batch loss: 0.34765326976776123\n",
            "Batch loss: 0.33105039596557617\n",
            "Batch loss: 0.4058697819709778\n",
            "Batch loss: 0.4267207384109497\n",
            "Batch loss: 0.258227676153183\n",
            "Batch loss: 0.3030526340007782\n",
            "Batch loss: 0.26570871472358704\n",
            "Batch loss: 0.6544628143310547\n",
            "Batch loss: 0.48428499698638916\n",
            "Batch loss: 0.42191842198371887\n",
            "Batch loss: 0.31372642517089844\n",
            "Batch loss: 0.3397676348686218\n",
            "Batch loss: 0.28602978587150574\n",
            "Batch loss: 0.3701018989086151\n",
            "Batch loss: 0.4856792986392975\n",
            "Batch loss: 0.6684739589691162\n",
            "Batch loss: 0.6501044034957886\n",
            "Batch loss: 0.7921481132507324\n",
            "Batch loss: 0.3700335919857025\n",
            "Batch loss: 0.29078105092048645\n",
            "Batch loss: 0.4282488226890564\n",
            "Batch loss: 0.7124269604682922\n",
            "Batch loss: 0.4136197865009308\n",
            "Batch loss: 0.5780330300331116\n",
            "Batch loss: 0.25010448694229126\n",
            "Batch loss: 0.3229721188545227\n",
            "Batch loss: 0.38804110884666443\n",
            "Batch loss: 0.8455279469490051\n",
            "Batch loss: 0.30603477358818054\n",
            "Batch loss: 0.441974937915802\n",
            "Batch loss: 0.5113094449043274\n",
            "Batch loss: 0.3940613269805908\n",
            "Batch loss: 0.3379656672477722\n",
            "Batch loss: 0.43694883584976196\n",
            "Batch loss: 0.31778785586357117\n",
            "Batch loss: 0.5076429843902588\n",
            "Batch loss: 0.39369142055511475\n",
            "Batch loss: 0.14339865744113922\n",
            "Batch loss: 0.23984156548976898\n",
            "Batch loss: 0.3952428102493286\n",
            "Batch loss: 0.3476593494415283\n",
            "Batch loss: 0.25350359082221985\n",
            "Batch loss: 0.3621819317340851\n",
            "Batch loss: 0.5796753764152527\n",
            "Batch loss: 0.41477128863334656\n",
            "Batch loss: 0.32930487394332886\n",
            "Batch loss: 0.3192289173603058\n",
            "Batch loss: 0.6866039633750916\n",
            "Batch loss: 0.4152384102344513\n",
            "Batch loss: 0.27370360493659973\n",
            "Batch loss: 0.17066402733325958\n",
            "Batch loss: 0.5694153308868408\n",
            "Batch loss: 0.2962021231651306\n",
            "Batch loss: 0.7255576848983765\n",
            "Batch loss: 0.3843144178390503\n",
            "Batch loss: 0.5201044082641602\n",
            "Batch loss: 0.46928295493125916\n",
            "Batch loss: 0.1454065889120102\n",
            "Batch loss: 0.3212801516056061\n",
            "Batch loss: 0.463002473115921\n",
            "Batch loss: 0.36759650707244873\n",
            "Batch loss: 0.3183974325656891\n",
            "Batch loss: 0.5595884919166565\n",
            "Batch loss: 0.2523133158683777\n",
            "Batch loss: 0.48727530241012573\n",
            "Batch loss: 0.5317254066467285\n",
            "Batch loss: 0.5020005702972412\n",
            "Batch loss: 0.43858566880226135\n",
            "Batch loss: 0.634515643119812\n",
            "Batch loss: 0.2708333730697632\n",
            "Batch loss: 0.34793731570243835\n",
            "Batch loss: 0.3108776807785034\n",
            "Batch loss: 0.47808656096458435\n",
            "Batch loss: 0.207633376121521\n",
            "Batch loss: 0.5470378994941711\n",
            "Batch loss: 0.3303706645965576\n",
            "Batch loss: 0.24493102729320526\n",
            "Batch loss: 0.41565296053886414\n",
            "Batch loss: 0.36896494030952454\n",
            "Batch loss: 0.20226287841796875\n",
            "Batch loss: 0.2635706961154938\n",
            "Batch loss: 0.5005253553390503\n",
            "Batch loss: 0.21585673093795776\n",
            "Batch loss: 0.3808094561100006\n",
            "Batch loss: 0.21240024268627167\n",
            "Batch loss: 0.4249984920024872\n",
            "Batch loss: 0.3614262342453003\n",
            "Batch loss: 0.5307302474975586\n",
            "Batch loss: 0.35925501585006714\n",
            "Batch loss: 0.5573793649673462\n",
            "Batch loss: 0.6127321720123291\n",
            "Batch loss: 0.13537287712097168\n",
            "Batch loss: 0.15418913960456848\n",
            "Batch loss: 0.4296482503414154\n",
            "Batch loss: 0.38031864166259766\n",
            "Batch loss: 0.4349737763404846\n",
            "Batch loss: 0.7066591382026672\n",
            "Batch loss: 0.41407471895217896\n",
            "Batch loss: 0.25884127616882324\n",
            "Batch loss: 0.3635745048522949\n",
            "Batch loss: 0.1238158643245697\n",
            "Batch loss: 0.38918739557266235\n",
            "Batch loss: 0.47763291001319885\n",
            "Batch loss: 0.4231003522872925\n",
            "Batch loss: 0.40764689445495605\n",
            "Batch loss: 0.6133648157119751\n",
            "Batch loss: 0.3335123062133789\n",
            "Batch loss: 0.45306795835494995\n",
            "Batch loss: 0.2207333743572235\n",
            "Batch loss: 0.4132925570011139\n",
            "Batch loss: 0.32998421788215637\n",
            "Batch loss: 0.5768806338310242\n",
            "Batch loss: 0.38031190633773804\n",
            "Batch loss: 0.3139694631099701\n",
            "Batch loss: 0.24548090994358063\n",
            "Batch loss: 0.6107522249221802\n",
            "Batch loss: 0.5329151749610901\n",
            "Batch loss: 0.31175222992897034\n",
            "Batch loss: 0.4001098871231079\n",
            "Batch loss: 0.2678945064544678\n",
            "Batch loss: 0.44297823309898376\n",
            "Batch loss: 0.21383589506149292\n",
            "Batch loss: 0.13727569580078125\n",
            "Batch loss: 0.34842076897621155\n",
            "Batch loss: 0.3549160361289978\n",
            "Batch loss: 0.5923010110855103\n",
            "Batch loss: 0.2443712204694748\n",
            "Batch loss: 0.25932151079177856\n",
            "Batch loss: 0.2831610441207886\n",
            "Batch loss: 0.2883569896221161\n",
            "Batch loss: 0.34905311465263367\n",
            "Batch loss: 0.4121414124965668\n",
            "Batch loss: 0.19468162953853607\n",
            "Batch loss: 0.08402007073163986\n",
            "Batch loss: 0.4540380835533142\n",
            "Batch loss: 0.40207338333129883\n",
            "Batch loss: 0.16351568698883057\n",
            "Batch loss: 0.38336318731307983\n",
            "Batch loss: 0.39234328269958496\n",
            "Batch loss: 0.13483047485351562\n",
            "Batch loss: 0.34437501430511475\n",
            "Batch loss: 0.1013789251446724\n",
            "Batch loss: 0.20493212342262268\n",
            "Batch loss: 0.4070180654525757\n",
            "Batch loss: 0.27292120456695557\n",
            "Batch loss: 0.6257856488227844\n",
            "Batch loss: 0.4375459849834442\n",
            "Batch loss: 0.24474741518497467\n",
            "Batch loss: 0.3730369508266449\n",
            "Batch loss: 0.2821539342403412\n",
            "Batch loss: 0.3212796747684479\n",
            "Batch loss: 0.35685840249061584\n",
            "Batch loss: 0.32888922095298767\n",
            "Batch loss: 0.32387402653694153\n",
            "Batch loss: 0.12093117088079453\n",
            "Batch loss: 0.3536759614944458\n",
            "Batch loss: 0.30298662185668945\n",
            "Batch loss: 0.35672527551651\n",
            "Batch loss: 0.32657697796821594\n",
            "Batch loss: 0.2652362883090973\n",
            "Batch loss: 0.3468932807445526\n",
            "Batch loss: 0.15011252462863922\n",
            "Batch loss: 0.45878496766090393\n",
            "Batch loss: 0.36556658148765564\n",
            "Batch loss: 0.1456371545791626\n",
            "Batch loss: 0.28514406085014343\n",
            "Batch loss: 0.2176603376865387\n",
            "Batch loss: 0.149765744805336\n",
            "Batch loss: 0.33275502920150757\n",
            "Batch loss: 0.15999098122119904\n",
            "Batch loss: 0.18365785479545593\n",
            "Batch loss: 0.2708902955055237\n",
            "Batch loss: 0.18945716321468353\n",
            "Batch loss: 0.3332306742668152\n",
            "Batch loss: 0.652423620223999\n",
            "Batch loss: 0.30255070328712463\n",
            "Batch loss: 0.255683571100235\n",
            "Batch loss: 0.1906767636537552\n",
            "Batch loss: 0.4813932478427887\n",
            "Batch loss: 0.5056308507919312\n",
            "Batch loss: 0.41888415813446045\n",
            "Batch loss: 0.27706438302993774\n",
            "Batch loss: 0.1822218894958496\n",
            "Batch loss: 0.184067502617836\n",
            "Batch loss: 0.3170289099216461\n",
            "Batch loss: 0.4487367272377014\n",
            "Batch loss: 0.5363524556159973\n",
            "Batch loss: 0.27108171582221985\n",
            "Batch loss: 0.3600466251373291\n",
            "Batch loss: 0.2357434630393982\n",
            "Batch loss: 0.5547760128974915\n",
            "Batch loss: 0.4100435972213745\n",
            "Batch loss: 0.1707519292831421\n",
            "Batch loss: 0.34615764021873474\n",
            "Batch loss: 0.42183494567871094\n",
            "Batch loss: 0.2494809627532959\n",
            "Batch loss: 0.23963189125061035\n",
            "Batch loss: 0.2889878451824188\n",
            "Batch loss: 0.242758646607399\n",
            "Batch loss: 0.2755253314971924\n",
            "Batch loss: 0.1516781747341156\n",
            "Batch loss: 0.4400821030139923\n",
            "Batch loss: 0.32830649614334106\n",
            "Batch loss: 0.4190892279148102\n",
            "Batch loss: 0.12673547863960266\n",
            "Batch loss: 0.5718379616737366\n",
            "Batch loss: 0.11954141408205032\n",
            "Batch loss: 0.36182406544685364\n",
            "Batch loss: 0.25306016206741333\n",
            "Batch loss: 0.4323720335960388\n",
            "Batch loss: 0.31205224990844727\n",
            "Batch loss: 0.2522961497306824\n",
            "Batch loss: 0.17643491923809052\n",
            "Batch loss: 0.2850789427757263\n",
            "Batch loss: 0.19992540776729584\n",
            "Batch loss: 0.2312067598104477\n",
            "Batch loss: 0.5621460676193237\n",
            "Batch loss: 0.32410725951194763\n",
            "Batch loss: 0.10446221381425858\n",
            "Batch loss: 0.5485936999320984\n",
            "Batch loss: 0.1892952024936676\n",
            "Batch loss: 0.3110349774360657\n",
            "Batch loss: 0.2950502336025238\n",
            "Batch loss: 0.3354724645614624\n",
            "Batch loss: 0.24626576900482178\n",
            "Batch loss: 0.2771309018135071\n",
            "Batch loss: 0.3018598258495331\n",
            "Batch loss: 0.4366515576839447\n",
            "Batch loss: 0.263301283121109\n",
            "Batch loss: 0.49510541558265686\n",
            "Batch loss: 0.49343734979629517\n",
            "Batch loss: 0.1659640371799469\n",
            "Batch loss: 0.2216857522726059\n",
            "Batch loss: 0.25281497836112976\n",
            "Batch loss: 0.35537731647491455\n",
            "Batch loss: 0.23064959049224854\n",
            "Batch loss: 0.23239120841026306\n",
            "Batch loss: 0.11304116249084473\n",
            "Batch loss: 0.3351864516735077\n",
            "Batch loss: 0.3716684579849243\n",
            "Batch loss: 0.19104477763175964\n",
            "Batch loss: 0.48278549313545227\n",
            "Batch loss: 0.2995986044406891\n",
            "Batch loss: 0.26890048384666443\n",
            "Batch loss: 0.11566426604986191\n",
            "Batch loss: 0.41331803798675537\n",
            "Batch loss: 0.27373865246772766\n",
            "Batch loss: 0.5647646188735962\n",
            "Batch loss: 0.30428627133369446\n",
            "Batch loss: 0.4474470913410187\n",
            "Batch loss: 0.15429100394248962\n",
            "Batch loss: 0.4252058267593384\n",
            "Batch loss: 0.24251040816307068\n",
            "Batch loss: 0.17142115533351898\n",
            "Batch loss: 0.5140387415885925\n",
            "Batch loss: 0.15727634727954865\n",
            "Batch loss: 0.45891493558883667\n",
            "Batch loss: 0.4620986878871918\n",
            "Batch loss: 0.3857173025608063\n",
            "Batch loss: 0.3040405213832855\n",
            "Batch loss: 0.3176732361316681\n",
            "Batch loss: 0.1583680361509323\n",
            "Batch loss: 0.4232287108898163\n",
            "Batch loss: 0.22579194605350494\n",
            "Batch loss: 0.1511652022600174\n",
            "Batch loss: 0.19525854289531708\n",
            "Batch loss: 0.2131287008523941\n",
            "Batch loss: 0.15432150661945343\n",
            "Batch loss: 0.19152241945266724\n",
            "Batch loss: 0.6349648237228394\n",
            "Batch loss: 0.4355987310409546\n",
            "Batch loss: 0.363686203956604\n",
            "Batch loss: 0.23378129303455353\n",
            "Batch loss: 0.5430682301521301\n",
            "Batch loss: 0.28065770864486694\n",
            "Batch loss: 0.22873464226722717\n",
            "Batch loss: 0.4174898862838745\n",
            "Batch loss: 0.4840852618217468\n",
            "Batch loss: 0.3692607581615448\n",
            "Batch loss: 0.16006669402122498\n",
            "Batch loss: 0.3925549387931824\n",
            "Batch loss: 0.19976414740085602\n",
            "Batch loss: 0.37716948986053467\n",
            "Batch loss: 0.2224874347448349\n",
            "Batch loss: 0.12308531254529953\n",
            "Batch loss: 0.12063289433717728\n",
            "Batch loss: 0.16483286023139954\n",
            "Batch loss: 0.6161946058273315\n",
            "Batch loss: 0.3899576663970947\n",
            "Batch loss: 0.3129138648509979\n",
            "Batch loss: 0.16887687146663666\n",
            "Batch loss: 0.16387024521827698\n",
            "Batch loss: 0.2792169153690338\n",
            "Batch loss: 0.1471772938966751\n",
            "Batch loss: 0.21725334227085114\n",
            "Batch loss: 0.23684822022914886\n",
            "Batch loss: 0.2278004288673401\n",
            "Batch loss: 0.15486828982830048\n",
            "Batch loss: 0.28973302245140076\n",
            "Batch loss: 0.12000007927417755\n",
            "Batch loss: 0.15228335559368134\n",
            "Batch loss: 0.14502258598804474\n",
            "Batch loss: 0.19488659501075745\n",
            "Batch loss: 0.49315565824508667\n",
            "Batch loss: 0.34470951557159424\n",
            "Batch loss: 0.3661469519138336\n",
            "Batch loss: 0.3162781000137329\n",
            "Batch loss: 0.18947677314281464\n",
            "Batch loss: 0.33206087350845337\n",
            "Batch loss: 0.252559095621109\n",
            "Batch loss: 0.43376123905181885\n",
            "Batch loss: 0.23409393429756165\n",
            "Batch loss: 0.3523310124874115\n",
            "Batch loss: 0.3042824864387512\n",
            "Batch loss: 0.29772084951400757\n",
            "Batch loss: 0.24980390071868896\n",
            "Batch loss: 0.2173464000225067\n",
            "Batch loss: 0.2882528305053711\n",
            "Batch loss: 0.0876040980219841\n",
            "Batch loss: 0.39450064301490784\n",
            "Batch loss: 0.2814021408557892\n",
            "Batch loss: 0.1738814264535904\n",
            "Batch loss: 0.2040843963623047\n",
            "Batch loss: 0.28327101469039917\n",
            "Batch loss: 0.553596019744873\n",
            "Batch loss: 0.30692148208618164\n",
            "Batch loss: 0.1361037939786911\n",
            "Batch loss: 0.4823099672794342\n",
            "Batch loss: 0.24225229024887085\n",
            "Batch loss: 0.283705472946167\n",
            "Batch loss: 0.07890326529741287\n",
            "Batch loss: 0.24839463829994202\n",
            "Batch loss: 0.15440838038921356\n",
            "Batch loss: 0.21928641200065613\n",
            "Batch loss: 0.45725804567337036\n",
            "Batch loss: 0.594648003578186\n",
            "Batch loss: 0.3672329783439636\n",
            "Batch loss: 0.18660159409046173\n",
            "Batch loss: 0.139085590839386\n",
            "Batch loss: 0.6863195300102234\n",
            "Batch loss: 0.2117573618888855\n",
            "Batch loss: 0.10367090255022049\n",
            "Batch loss: 0.07597005367279053\n",
            "Batch loss: 0.23022757470607758\n",
            "Batch loss: 0.49372997879981995\n",
            "Batch loss: 0.13234563171863556\n",
            "Batch loss: 0.28910505771636963\n",
            "Batch loss: 0.3138236105442047\n",
            "Batch loss: 0.3421567976474762\n",
            "Batch loss: 0.14235617220401764\n",
            "Batch loss: 0.13817863166332245\n",
            "Batch loss: 0.1506291627883911\n",
            "Batch loss: 0.24405913054943085\n",
            "Batch loss: 0.32830357551574707\n",
            "Batch loss: 0.30250799655914307\n",
            "Batch loss: 0.44601118564605713\n",
            "Batch loss: 0.3736626207828522\n",
            "Batch loss: 0.6754646897315979\n",
            "Batch loss: 0.27041691541671753\n",
            "Batch loss: 0.3238343298435211\n",
            "Batch loss: 0.4976586103439331\n",
            "Batch loss: 0.13937589526176453\n",
            "Batch loss: 0.2644951641559601\n",
            "Batch loss: 0.3808777630329132\n",
            "Batch loss: 0.10571863502264023\n",
            "Batch loss: 0.24895144999027252\n",
            "Batch loss: 0.28531596064567566\n",
            "Batch loss: 0.4521629512310028\n",
            "Batch loss: 0.27165845036506653\n",
            "Batch loss: 0.13495174050331116\n",
            "Batch loss: 0.3292595148086548\n",
            "Batch loss: 0.26566267013549805\n",
            "Batch loss: 0.3677348494529724\n",
            "Batch loss: 0.16901364922523499\n",
            "Batch loss: 0.18584902584552765\n",
            "Batch loss: 0.18405234813690186\n",
            "Batch loss: 0.47730591893196106\n",
            "Batch loss: 0.2778860628604889\n",
            "Batch loss: 0.3147999346256256\n",
            "Batch loss: 0.17084501683712006\n",
            "Batch loss: 0.27323418855667114\n",
            "Batch loss: 0.19264332950115204\n",
            "Batch loss: 0.2870834767818451\n",
            "Batch loss: 0.34120747447013855\n",
            "Batch loss: 0.12132568657398224\n",
            "Batch loss: 0.3365684151649475\n",
            "Batch loss: 0.16808725893497467\n",
            "Batch loss: 0.17950353026390076\n",
            "Batch loss: 0.3980129361152649\n",
            "Batch loss: 0.14959494769573212\n",
            "Batch loss: 0.17644467949867249\n",
            "Batch loss: 0.488170325756073\n",
            "Batch loss: 0.3672647178173065\n",
            "Batch loss: 0.17731894552707672\n",
            "Batch loss: 0.14567475020885468\n",
            "Batch loss: 0.18912823498249054\n",
            "Batch loss: 0.07308551669120789\n",
            "Batch loss: 0.3743554651737213\n",
            "Batch loss: 0.18742983043193817\n",
            "Batch loss: 0.39433732628822327\n",
            "Batch loss: 0.16623175144195557\n",
            "Batch loss: 0.18120409548282623\n",
            "Batch loss: 0.05569969117641449\n",
            "Batch loss: 0.5506200194358826\n",
            "Batch loss: 0.42619970440864563\n",
            "Batch loss: 0.41342392563819885\n",
            "Batch loss: 0.15278765559196472\n",
            "Batch loss: 0.24929116666316986\n",
            "Batch loss: 0.3643385171890259\n",
            "Batch loss: 0.35640817880630493\n",
            "Batch loss: 0.312251478433609\n",
            "Batch loss: 0.13097591698169708\n",
            "Batch loss: 0.15090392529964447\n",
            "Batch loss: 0.2916891574859619\n",
            "Batch loss: 0.17362888157367706\n",
            "Batch loss: 0.3002214729785919\n",
            "Batch loss: 0.19159546494483948\n",
            "Batch loss: 0.2464076429605484\n",
            "Batch loss: 0.2594699263572693\n",
            "Batch loss: 0.10211411863565445\n",
            "Batch loss: 0.16065733134746552\n",
            "Batch loss: 0.1423681229352951\n",
            "Batch loss: 0.3715842664241791\n",
            "Batch loss: 0.16838905215263367\n",
            "Batch loss: 0.21468549966812134\n",
            "Batch loss: 0.3767179846763611\n",
            "Batch loss: 0.21284182369709015\n",
            "Batch loss: 0.1509193629026413\n",
            "Batch loss: 0.4002165198326111\n",
            "Batch loss: 0.10918249189853668\n",
            "Batch loss: 0.4068797528743744\n",
            "Batch loss: 0.10381041467189789\n",
            "Batch loss: 0.12507477402687073\n",
            "Batch loss: 0.2813522219657898\n",
            "Batch loss: 0.30696338415145874\n",
            "Batch loss: 0.17053140699863434\n",
            "Batch loss: 0.25415387749671936\n",
            "Batch loss: 0.7061591148376465\n",
            "Batch loss: 0.1939106583595276\n",
            "Batch loss: 0.20911674201488495\n",
            "Batch loss: 0.4467850625514984\n",
            "Batch loss: 0.15930898487567902\n",
            "Batch loss: 0.42314550280570984\n",
            "Batch loss: 0.3274267613887787\n",
            "Batch loss: 0.4433358907699585\n",
            "Batch loss: 0.1606256663799286\n",
            "Batch loss: 0.2524632513523102\n",
            "Batch loss: 0.1029362753033638\n",
            "Batch loss: 0.2634684443473816\n",
            "Batch loss: 0.3910841941833496\n",
            "Batch loss: 0.23220737278461456\n",
            "Batch loss: 0.5234028100967407\n",
            "Batch loss: 0.42254725098609924\n",
            "Batch loss: 0.2147846668958664\n",
            "Batch loss: 0.19510100781917572\n",
            "Batch loss: 0.2660394608974457\n",
            "Batch loss: 0.2882313132286072\n",
            "Batch loss: 0.13704749941825867\n",
            "Batch loss: 0.15966258943080902\n",
            "Batch loss: 0.21104854345321655\n",
            "Batch loss: 0.073076531291008\n",
            "Batch loss: 0.11169861257076263\n",
            "Batch loss: 0.17072699964046478\n",
            "Batch loss: 0.31560850143432617\n",
            "Batch loss: 0.17104360461235046\n",
            "Batch loss: 0.2871183454990387\n",
            "Batch loss: 0.274314284324646\n",
            "Batch loss: 0.4969583749771118\n",
            "Batch loss: 0.5839415192604065\n",
            "Batch loss: 0.15641257166862488\n",
            "Batch loss: 0.16438628733158112\n",
            "Batch loss: 0.6454793810844421\n",
            "Batch loss: 0.2944992184638977\n",
            "Batch loss: 0.26078811287879944\n",
            "Batch loss: 0.14477767050266266\n",
            "Batch loss: 0.32741671800613403\n",
            "Batch loss: 0.6158274412155151\n",
            "Batch loss: 0.38351330161094666\n",
            "Batch loss: 0.4228503704071045\n",
            "Batch loss: 0.30056658387184143\n",
            "Batch loss: 0.28657373785972595\n",
            "Batch loss: 0.2920681834220886\n",
            "Batch loss: 0.2035343497991562\n",
            "Batch loss: 0.28701964020729065\n",
            "Batch loss: 0.10943053662776947\n",
            "Batch loss: 0.2787470519542694\n",
            "Batch loss: 0.18168720602989197\n",
            "Batch loss: 0.22696295380592346\n",
            "Batch loss: 0.26868700981140137\n",
            "Batch loss: 0.30094820261001587\n",
            "Batch loss: 0.23605993390083313\n",
            "Batch loss: 0.2433500587940216\n",
            "Batch loss: 0.23252251744270325\n",
            "Batch loss: 0.47412624955177307\n",
            "Batch loss: 0.19004996120929718\n",
            "Batch loss: 0.2605917453765869\n",
            "Batch loss: 0.11387380212545395\n",
            "Batch loss: 0.2764095962047577\n",
            "Batch loss: 0.25759264826774597\n",
            "Batch loss: 0.11989516019821167\n",
            "Batch loss: 0.2546929121017456\n",
            "Batch loss: 0.31257164478302\n",
            "Batch loss: 0.3226313591003418\n",
            "Batch loss: 0.2035648077726364\n",
            "Batch loss: 0.12950116395950317\n",
            "Batch loss: 0.1571422964334488\n",
            "Batch loss: 0.31888705492019653\n",
            "Batch loss: 0.1534142643213272\n",
            "Batch loss: 0.41870859265327454\n",
            "Batch loss: 0.2578835189342499\n",
            "Batch loss: 0.24783018231391907\n",
            "Batch loss: 0.15203136205673218\n",
            "Batch loss: 0.05260905250906944\n",
            "Batch loss: 0.3002946972846985\n",
            "Batch loss: 0.2828906774520874\n",
            "Batch loss: 0.3419257700443268\n",
            "Batch loss: 0.17314770817756653\n",
            "Batch loss: 0.19834555685520172\n",
            "Batch loss: 0.2997853457927704\n",
            "Batch loss: 0.6528033018112183\n",
            "Batch loss: 0.15573324263095856\n",
            "Batch loss: 0.14209343492984772\n",
            "Batch loss: 0.2542732357978821\n",
            "Batch loss: 0.19504694640636444\n",
            "Batch loss: 0.21908706426620483\n",
            "Batch loss: 0.2785945534706116\n",
            "Batch loss: 0.3428705036640167\n",
            "Batch loss: 0.3686301112174988\n",
            "Batch loss: 0.1754835993051529\n",
            "Batch loss: 0.24961215257644653\n",
            "Batch loss: 0.3325599730014801\n",
            "Batch loss: 0.23295745253562927\n",
            "Batch loss: 0.2613222599029541\n",
            "Batch loss: 0.19280503690242767\n",
            "Batch loss: 0.3822867274284363\n",
            "Batch loss: 0.20952503383159637\n",
            "Batch loss: 0.23307228088378906\n",
            "Batch loss: 0.16284461319446564\n",
            "Batch loss: 0.15426413714885712\n",
            "Batch loss: 0.1872778981924057\n",
            "Batch loss: 0.36522480845451355\n",
            "Batch loss: 0.1901998221874237\n",
            "Batch loss: 0.30194535851478577\n",
            "Batch loss: 0.28822001814842224\n",
            "Batch loss: 0.09999711811542511\n",
            "Batch loss: 0.2940078675746918\n",
            "Batch loss: 0.1579139083623886\n",
            "Batch loss: 0.26285499334335327\n",
            "Batch loss: 0.24060195684432983\n",
            "Batch loss: 0.22726845741271973\n",
            "Batch loss: 0.30080053210258484\n",
            "Batch loss: 0.20041467249393463\n",
            "Batch loss: 0.047146961092948914\n",
            "Batch loss: 0.2300870716571808\n",
            "Batch loss: 0.18502819538116455\n",
            "Batch loss: 0.29291173815727234\n",
            "Batch loss: 0.3482690155506134\n",
            "Batch loss: 0.2872081696987152\n",
            "Batch loss: 0.1710890382528305\n",
            "Batch loss: 0.2438555210828781\n",
            "Batch loss: 0.18430377542972565\n",
            "Batch loss: 0.2936558723449707\n",
            "Batch loss: 0.2857658863067627\n",
            "Batch loss: 0.19397222995758057\n",
            "Batch loss: 0.39887771010398865\n",
            "Batch loss: 0.2230994552373886\n",
            "Batch loss: 0.21159708499908447\n",
            "Batch loss: 0.13957856595516205\n",
            "Batch loss: 0.18240155279636383\n",
            "Batch loss: 0.16574570536613464\n",
            "Batch loss: 0.29404574632644653\n",
            "Batch loss: 0.04339691251516342\n",
            "Batch loss: 0.17761653661727905\n",
            "Batch loss: 0.2748938798904419\n",
            "Batch loss: 0.20179377496242523\n",
            "Batch loss: 0.20117545127868652\n",
            "Batch loss: 0.471580445766449\n",
            "Batch loss: 0.2381952404975891\n",
            "Batch loss: 0.07496809959411621\n",
            "Batch loss: 0.4004491865634918\n",
            "Batch loss: 0.3004612922668457\n",
            "Batch loss: 0.2481023222208023\n",
            "Batch loss: 0.18271289765834808\n",
            "Batch loss: 0.3829895853996277\n",
            "Batch loss: 0.26108884811401367\n",
            "Batch loss: 0.18818756937980652\n",
            "Batch loss: 0.1575407236814499\n",
            "Batch loss: 0.24673713743686676\n",
            "Batch loss: 0.06988949328660965\n",
            "Batch loss: 0.17517928779125214\n",
            "Batch loss: 0.36612969636917114\n",
            "Batch loss: 0.18672098219394684\n",
            "Batch loss: 0.44109824299812317\n",
            "Batch loss: 0.06061491370201111\n",
            "Batch loss: 0.21562159061431885\n",
            "Batch loss: 0.11628003418445587\n",
            "Batch loss: 0.23313526809215546\n",
            "Batch loss: 0.5564735531806946\n",
            "Batch loss: 0.24622510373592377\n",
            "Batch loss: 0.21109996736049652\n",
            "Batch loss: 0.2527282238006592\n",
            "Batch loss: 0.11496078968048096\n",
            "Batch loss: 0.2692308723926544\n",
            "Batch loss: 0.33786097168922424\n",
            "Batch loss: 0.19254854321479797\n",
            "Batch loss: 0.11597039550542831\n",
            "Batch loss: 0.14889800548553467\n",
            "Batch loss: 0.09291417896747589\n",
            "Batch loss: 0.21969537436962128\n",
            "Batch loss: 0.33704957365989685\n",
            "Batch loss: 0.22884558141231537\n",
            "Batch loss: 0.20964111387729645\n",
            "Batch loss: 0.24374030530452728\n",
            "Batch loss: 0.16364547610282898\n",
            "Batch loss: 0.1536720097064972\n",
            "Batch loss: 0.2302820086479187\n",
            "Batch loss: 0.33018386363983154\n",
            "Batch loss: 0.17854274809360504\n",
            "Batch loss: 0.12757737934589386\n",
            "Batch loss: 0.2101745903491974\n",
            "Batch loss: 0.29570290446281433\n",
            "Batch loss: 0.24345429241657257\n",
            "Batch loss: 0.25736570358276367\n",
            "Batch loss: 0.20364899933338165\n",
            "Batch loss: 0.2749662697315216\n",
            "Batch loss: 0.19256770610809326\n",
            "Batch loss: 0.2690737247467041\n",
            "Batch loss: 0.3597670793533325\n",
            "Batch loss: 0.2647978365421295\n",
            "Batch loss: 0.2028573602437973\n",
            "Batch loss: 0.2414666712284088\n",
            "Batch loss: 0.271720290184021\n",
            "Batch loss: 0.250082790851593\n",
            "Batch loss: 0.09144939482212067\n",
            "Batch loss: 0.39709553122520447\n",
            "Batch loss: 0.11914265155792236\n",
            "Batch loss: 0.2545005679130554\n",
            "Batch loss: 0.2772753834724426\n",
            "Batch loss: 0.2696819007396698\n",
            "Batch loss: 0.17773063480854034\n",
            "Batch loss: 0.10841342806816101\n",
            "Batch loss: 0.1918637752532959\n",
            "Batch loss: 0.2641064524650574\n",
            "Batch loss: 0.2140897959470749\n",
            "Batch loss: 0.28081759810447693\n",
            "Batch loss: 0.0819481834769249\n",
            "Batch loss: 0.12553609907627106\n",
            "Batch loss: 0.3553794026374817\n",
            "Batch loss: 0.10122409462928772\n",
            "Batch loss: 0.2645013928413391\n",
            "Batch loss: 0.1697436273097992\n",
            "Batch loss: 0.2979397475719452\n",
            "Batch loss: 0.4343205690383911\n",
            "Batch loss: 0.18307210505008698\n",
            "Batch loss: 0.29424116015434265\n",
            "Batch loss: 0.26265034079551697\n",
            "Batch loss: 0.24282468855381012\n",
            "Batch loss: 0.22408191859722137\n",
            "Batch loss: 0.22111865878105164\n",
            "Batch loss: 0.18728011846542358\n",
            "Batch loss: 0.38611388206481934\n",
            "Batch loss: 0.12208729982376099\n",
            "Batch loss: 0.14663158357143402\n",
            "Batch loss: 0.2662079334259033\n",
            "Batch loss: 0.12920205295085907\n",
            "Batch loss: 0.22140473127365112\n",
            "Batch loss: 0.18152689933776855\n",
            "Batch loss: 0.3339729905128479\n",
            "Batch loss: 0.4782077670097351\n",
            "Batch loss: 0.19678421318531036\n",
            "Batch loss: 0.11701920628547668\n",
            "Batch loss: 0.27574604749679565\n",
            "Batch loss: 0.14255863428115845\n",
            "Batch loss: 0.08009824901819229\n",
            "Batch loss: 0.13043735921382904\n",
            "Batch loss: 0.2567930221557617\n",
            "Batch loss: 0.2780824303627014\n",
            "Batch loss: 0.24279850721359253\n",
            "Batch loss: 0.23364989459514618\n",
            "Batch loss: 0.12916839122772217\n",
            "Batch loss: 0.31873881816864014\n",
            "Batch loss: 0.465698778629303\n",
            "Batch loss: 0.0851680114865303\n",
            "Batch loss: 0.29200479388237\n",
            "Batch loss: 0.03807952627539635\n",
            "Batch loss: 0.4648531675338745\n",
            "Batch loss: 0.5020814538002014\n",
            "Batch loss: 0.22972306609153748\n",
            "Batch loss: 0.22613345086574554\n",
            "Batch loss: 0.25900277495384216\n",
            "Batch loss: 0.6193380951881409\n",
            "Batch loss: 0.1277964860200882\n",
            "Batch loss: 0.15817397832870483\n",
            "Batch loss: 0.09703555703163147\n",
            "Batch loss: 0.1907220482826233\n",
            "Batch loss: 0.1561008244752884\n",
            "Batch loss: 0.20335833728313446\n",
            "Batch loss: 0.08944915980100632\n",
            "Batch loss: 0.40058183670043945\n",
            "Batch loss: 0.476798951625824\n",
            "Batch loss: 0.28411880135536194\n",
            "Batch loss: 0.15598107874393463\n",
            "Batch loss: 0.20877031981945038\n",
            "Batch loss: 0.33389154076576233\n",
            "Batch loss: 0.17359933257102966\n",
            "Batch loss: 0.19702544808387756\n",
            "Batch loss: 0.1113656535744667\n",
            "Batch loss: 0.3873835802078247\n",
            "Batch loss: 0.21393318474292755\n",
            "Batch loss: 0.24658885598182678\n",
            "Batch loss: 0.11563917249441147\n",
            "Batch loss: 0.1555391550064087\n",
            "Batch loss: 0.192045658826828\n",
            "Batch loss: 0.13474255800247192\n",
            "Batch loss: 0.14686530828475952\n",
            "Batch loss: 0.43003225326538086\n",
            "Batch loss: 0.23271165788173676\n",
            "Batch loss: 0.20545543730258942\n",
            "Batch loss: 0.30398982763290405\n",
            "Batch loss: 0.384922593832016\n",
            "Batch loss: 0.1937718689441681\n",
            "Batch loss: 0.1617913395166397\n",
            "Batch loss: 0.16471967101097107\n",
            "Batch loss: 0.16835275292396545\n",
            "Batch loss: 0.1698828786611557\n",
            "Batch loss: 0.14898158609867096\n",
            "Batch loss: 0.18657465279102325\n",
            "Batch loss: 0.19527697563171387\n",
            "Batch loss: 0.17604653537273407\n",
            "Batch loss: 0.20897284150123596\n",
            "Batch loss: 0.19769680500030518\n",
            "Batch loss: 0.09014290571212769\n",
            "Batch loss: 0.2531425952911377\n",
            "Batch loss: 0.21779000759124756\n",
            "Batch loss: 0.2037457674741745\n",
            "Batch loss: 0.28891506791114807\n",
            "Batch loss: 0.2120567113161087\n",
            "Batch loss: 0.1585678607225418\n",
            "Batch loss: 0.23059868812561035\n",
            "Batch loss: 0.15263870358467102\n",
            "Batch loss: 0.22952774167060852\n",
            "Batch loss: 0.2361215502023697\n",
            "Batch loss: 0.08566823601722717\n",
            "Batch loss: 0.24516107141971588\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "if input(\"Are you sure you want to begin training and saving models? Select (Y/N): \") != \"Y\":\n",
        "    exit()\n",
        "else:\n",
        "  model.train()\n",
        "  #inputs, labels = next(iter(train_loader))\n",
        "  for epoch in range(num_epochs):\n",
        "    for j, data in enumerate(train_loader, start=0):\n",
        "\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs) #forwards\n",
        "      loss = criterion(outputs, labels) #calculate loss\n",
        "      optimizer.zero_grad() #zero the parameter gradients\n",
        "      loss.backward() #backwards\n",
        "      optimizer.step() #optimize\n",
        "\n",
        "      print(f'Batch loss: {loss.item()}')\n",
        "\n",
        "      #scheduler.step()\n",
        "  #https://stackoverflow.com/questions/32490629/getting-todays-date-in-yyyy-mm-dd-in-python\n",
        "  today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "  path = f'/content/drive/MyDrive/{today}_model_2.pth'\n",
        "  torch.save(model.state_dict(), path)\n",
        "  print('Model saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu7hyUOCdbLy"
      },
      "source": [
        "Continue training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxdUFr3HdW9Z",
        "outputId": "760700e0-7a16-47b2-bd34-362ae1667fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Are you sure you want to continue training and saving models? Select (Y/N): Y\n",
            "How many more epochs?: 2\n",
            "Start of epoch\n",
            "Batch loss: 0.03317996859550476\n",
            "Batch loss: 0.0019977388437837362\n",
            "Batch loss: 0.0057356394827365875\n",
            "Batch loss: 0.009362767450511456\n",
            "Batch loss: 0.07775802165269852\n",
            "Batch loss: 0.00397285632789135\n",
            "Batch loss: 0.0021896350663155317\n",
            "Batch loss: 0.003714280901476741\n",
            "Batch loss: 0.029969472438097\n",
            "Batch loss: 0.0021305298432707787\n",
            "Batch loss: 0.11610826104879379\n",
            "Batch loss: 0.01308840699493885\n",
            "Batch loss: 0.006084202788770199\n",
            "Batch loss: 0.0012797724921256304\n",
            "Batch loss: 0.07253751903772354\n",
            "Batch loss: 0.006484930869191885\n",
            "Batch loss: 0.008894371800124645\n",
            "Batch loss: 0.05145562067627907\n",
            "Batch loss: 0.03641821816563606\n",
            "Batch loss: 0.01646299473941326\n",
            "Batch loss: 0.016747137531638145\n",
            "Batch loss: 0.0015456556575372815\n",
            "Batch loss: 0.0010858927853405476\n",
            "Batch loss: 0.028108375146985054\n",
            "Batch loss: 0.04307341203093529\n",
            "Batch loss: 0.012176466174423695\n",
            "Batch loss: 0.1545790731906891\n",
            "Batch loss: 0.13949818909168243\n",
            "Batch loss: 0.030737636610865593\n",
            "Batch loss: 0.003685807576403022\n",
            "Batch loss: 0.008968563750386238\n",
            "Batch loss: 0.07144615799188614\n",
            "Batch loss: 0.12024146318435669\n",
            "Batch loss: 0.002946453634649515\n",
            "Batch loss: 0.005566400475800037\n",
            "Batch loss: 0.014341354370117188\n",
            "Batch loss: 0.0088946633040905\n",
            "Batch loss: 0.0004218717513140291\n",
            "Batch loss: 0.015657056123018265\n",
            "Batch loss: 0.04272021725773811\n",
            "Batch loss: 0.0021316821221262217\n",
            "Batch loss: 0.06491590291261673\n",
            "Batch loss: 0.03819780796766281\n",
            "Batch loss: 0.0747486799955368\n",
            "Batch loss: 0.011453111656010151\n",
            "Batch loss: 0.03158801421523094\n",
            "Batch loss: 0.0059454310685396194\n",
            "Batch loss: 0.040984515100717545\n",
            "Batch loss: 0.006331331562250853\n",
            "Batch loss: 0.023549465462565422\n",
            "Batch loss: 0.0055086915381252766\n",
            "Batch loss: 0.024968963116407394\n",
            "Batch loss: 0.0033580828458070755\n",
            "Batch loss: 0.07321251928806305\n",
            "Batch loss: 0.06052551046013832\n",
            "Batch loss: 0.013718711212277412\n",
            "Batch loss: 0.03491220623254776\n",
            "Batch loss: 0.0019052098505198956\n",
            "Batch loss: 0.02287876605987549\n",
            "Batch loss: 0.010874344035983086\n",
            "Batch loss: 0.01667613536119461\n",
            "Batch loss: 0.005780740175396204\n",
            "Batch loss: 0.08068971335887909\n",
            "Batch loss: 0.03327024728059769\n",
            "Batch loss: 0.0016236638184636831\n",
            "Batch loss: 0.01456177793443203\n",
            "Batch loss: 0.0027059067506343126\n",
            "Batch loss: 0.059571344405412674\n",
            "Batch loss: 0.023236243054270744\n",
            "Batch loss: 0.005231914576143026\n",
            "Batch loss: 0.04046541824936867\n",
            "Batch loss: 0.021348968148231506\n",
            "Batch loss: 0.005393305793404579\n",
            "Batch loss: 0.0025357194244861603\n",
            "Batch loss: 0.003822334809228778\n",
            "Batch loss: 0.013510972261428833\n",
            "Batch loss: 0.005466071423143148\n",
            "Batch loss: 0.01958627440035343\n",
            "Batch loss: 0.018789874389767647\n",
            "Batch loss: 0.006817955523729324\n",
            "Batch loss: 0.01089756190776825\n",
            "Batch loss: 0.01759333722293377\n",
            "Batch loss: 0.02079194411635399\n",
            "Batch loss: 0.010243565775454044\n",
            "Batch loss: 0.09713646024465561\n",
            "Batch loss: 0.045675188302993774\n",
            "Batch loss: 0.015915410593152046\n",
            "Batch loss: 0.0027875856030732393\n",
            "Batch loss: 0.009340743534266949\n",
            "Batch loss: 0.003715827828273177\n",
            "Batch loss: 0.02767661213874817\n",
            "Batch loss: 0.02406122535467148\n",
            "Batch loss: 0.0016265506856143475\n",
            "Batch loss: 0.03522706404328346\n",
            "Batch loss: 0.003740503918379545\n",
            "Batch loss: 0.07573482394218445\n",
            "Batch loss: 0.018496990203857422\n",
            "Batch loss: 0.0021119737066328526\n",
            "Batch loss: 0.004668666049838066\n",
            "Batch loss: 0.02813870832324028\n",
            "Batch loss: 0.00563175929710269\n",
            "Batch loss: 0.04885020852088928\n",
            "Batch loss: 0.010256518609821796\n",
            "Batch loss: 0.08016351610422134\n",
            "Batch loss: 0.0040689557790756226\n",
            "Batch loss: 0.018305614590644836\n",
            "Batch loss: 0.011424580588936806\n",
            "Batch loss: 0.19616015255451202\n",
            "Batch loss: 0.008118629455566406\n",
            "Batch loss: 0.00748615013435483\n",
            "Batch loss: 0.028427913784980774\n",
            "Batch loss: 0.007735212333500385\n",
            "Batch loss: 0.01392086036503315\n",
            "Batch loss: 0.0013243190478533506\n",
            "Batch loss: 0.043839726597070694\n",
            "Batch loss: 0.009599638171494007\n",
            "Batch loss: 0.011004333384335041\n",
            "Batch loss: 0.18794839084148407\n",
            "Batch loss: 0.01732204109430313\n",
            "Batch loss: 0.033844709396362305\n",
            "Batch loss: 0.1303931027650833\n",
            "Batch loss: 0.001148832612670958\n",
            "Batch loss: 0.04005983844399452\n",
            "Batch loss: 0.001974253449589014\n",
            "Batch loss: 0.01336026843637228\n",
            "Batch loss: 0.10400022566318512\n",
            "Batch loss: 0.012529565021395683\n",
            "Batch loss: 0.010851947590708733\n",
            "Batch loss: 0.0207155030220747\n",
            "Batch loss: 0.0013482230715453625\n",
            "Batch loss: 0.016250470653176308\n",
            "Batch loss: 0.009087305516004562\n",
            "Batch loss: 0.00694128917530179\n",
            "Batch loss: 0.07482168078422546\n",
            "Batch loss: 0.006341204047203064\n",
            "Batch loss: 0.027934646233916283\n",
            "Batch loss: 0.017612535506486893\n",
            "Batch loss: 0.0056193177588284016\n",
            "Batch loss: 0.0448368676006794\n",
            "Batch loss: 0.0124942222610116\n",
            "Batch loss: 0.032556235790252686\n",
            "Batch loss: 0.0204300619661808\n",
            "Batch loss: 0.01914430409669876\n",
            "Batch loss: 0.060402851551771164\n",
            "Batch loss: 0.05211266130208969\n",
            "Batch loss: 0.039306145161390305\n",
            "Batch loss: 0.0016835251590237021\n",
            "Batch loss: 0.006564973387867212\n",
            "Batch loss: 0.022373568266630173\n",
            "Batch loss: 0.004304778296500444\n",
            "Batch loss: 0.004427521489560604\n",
            "Batch loss: 0.03381868451833725\n",
            "Batch loss: 0.054976265877485275\n",
            "Batch loss: 0.03655334934592247\n",
            "Batch loss: 0.0301887858659029\n",
            "Batch loss: 0.0026176057290285826\n",
            "Batch loss: 0.013887865468859673\n",
            "Batch loss: 0.011503119952976704\n",
            "Batch loss: 0.013565825298428535\n",
            "Batch loss: 0.01579018495976925\n",
            "Batch loss: 0.02591436170041561\n",
            "Batch loss: 0.037625979632139206\n",
            "Batch loss: 0.06881696730852127\n",
            "Batch loss: 0.004136018455028534\n",
            "Batch loss: 0.05412594974040985\n",
            "Batch loss: 0.013170044869184494\n",
            "Batch loss: 0.035643815994262695\n",
            "Batch loss: 0.0022961210925132036\n",
            "Batch loss: 0.013375598005950451\n",
            "Batch loss: 0.0360463485121727\n",
            "Batch loss: 0.002357193036004901\n",
            "Batch loss: 0.028991574421525\n",
            "Batch loss: 0.010594138875603676\n",
            "Batch loss: 0.0010546264238655567\n",
            "Batch loss: 0.028714347630739212\n",
            "Batch loss: 0.014070163480937481\n",
            "Batch loss: 0.046257734298706055\n",
            "Batch loss: 0.0454404391348362\n",
            "Batch loss: 0.03594159334897995\n",
            "Batch loss: 0.021848097443580627\n",
            "Batch loss: 0.03118220716714859\n",
            "Batch loss: 0.06077906861901283\n",
            "Batch loss: 0.006661252584308386\n",
            "Batch loss: 0.003097689710557461\n",
            "Batch loss: 0.01576445996761322\n",
            "Batch loss: 0.003357936628162861\n",
            "Batch loss: 0.009625325910747051\n",
            "Batch loss: 0.011835342273116112\n",
            "Batch loss: 0.1201782077550888\n",
            "Batch loss: 0.013237247243523598\n",
            "Batch loss: 0.006805054377764463\n",
            "Batch loss: 0.04275887832045555\n",
            "Batch loss: 0.010475389659404755\n",
            "Batch loss: 0.03549440950155258\n",
            "Batch loss: 0.09757369011640549\n",
            "Batch loss: 0.05680980160832405\n",
            "Batch loss: 0.00362722878344357\n",
            "Batch loss: 0.04389113187789917\n",
            "Batch loss: 0.07874323427677155\n",
            "Batch loss: 0.06595935672521591\n",
            "Batch loss: 0.007058554794639349\n",
            "Batch loss: 0.06164674460887909\n",
            "Batch loss: 0.004035713151097298\n",
            "Batch loss: 0.12596899271011353\n",
            "Batch loss: 0.013730566017329693\n",
            "Batch loss: 0.003061435418203473\n",
            "Batch loss: 0.0336163230240345\n",
            "Batch loss: 0.05165587365627289\n",
            "Batch loss: 0.10017981380224228\n",
            "Batch loss: 0.07853401452302933\n",
            "Batch loss: 0.05895974114537239\n",
            "Batch loss: 0.08083689212799072\n",
            "Batch loss: 0.049294229596853256\n",
            "Batch loss: 0.022895220667123795\n",
            "Batch loss: 0.00862551387399435\n",
            "Batch loss: 0.03776304051280022\n",
            "Batch loss: 0.0279279425740242\n",
            "Batch loss: 0.06631939113140106\n",
            "Batch loss: 0.04730181768536568\n",
            "Batch loss: 0.026438355445861816\n",
            "Batch loss: 0.021723691374063492\n",
            "Batch loss: 0.008721391670405865\n",
            "Batch loss: 0.027882909402251244\n",
            "Batch loss: 0.12402360141277313\n",
            "Batch loss: 0.007192492950707674\n",
            "Batch loss: 0.12447400391101837\n",
            "Batch loss: 0.04783983901143074\n",
            "Batch loss: 0.0019015147117897868\n",
            "Batch loss: 0.01689249277114868\n",
            "Batch loss: 0.0023087107110768557\n",
            "Batch loss: 0.031405530869960785\n",
            "Batch loss: 0.07372651249170303\n",
            "Batch loss: 0.048716552555561066\n",
            "Batch loss: 0.02674318477511406\n",
            "Batch loss: 0.063604936003685\n",
            "Batch loss: 0.08483292907476425\n",
            "Batch loss: 0.013963501900434494\n",
            "Batch loss: 0.013942901045084\n",
            "Batch loss: 0.0023993055801838636\n",
            "Batch loss: 0.012414585798978806\n",
            "Batch loss: 0.006011078599840403\n",
            "Batch loss: 0.015129963867366314\n",
            "Batch loss: 0.002251602942124009\n",
            "Batch loss: 0.07700659334659576\n",
            "Batch loss: 0.02017594315111637\n",
            "Batch loss: 0.024631317704916\n",
            "Batch loss: 0.00618225522339344\n",
            "Batch loss: 0.0057486058212816715\n",
            "Batch loss: 0.008334070444107056\n",
            "Batch loss: 0.028841685503721237\n",
            "Batch loss: 0.017523018643260002\n",
            "Batch loss: 0.0027630485128611326\n",
            "Batch loss: 0.006611188407987356\n",
            "Batch loss: 0.050831522792577744\n",
            "Batch loss: 0.037683550268411636\n",
            "Batch loss: 0.12568975985050201\n",
            "Batch loss: 0.05224156752228737\n",
            "Batch loss: 0.12591300904750824\n",
            "Batch loss: 0.004514729604125023\n",
            "Batch loss: 0.0025082831270992756\n",
            "Batch loss: 0.015291894786059856\n",
            "Batch loss: 0.01080353930592537\n",
            "Batch loss: 0.01702844351530075\n",
            "Batch loss: 0.043715354055166245\n",
            "Batch loss: 0.007684611715376377\n",
            "Batch loss: 0.014697853475809097\n",
            "Batch loss: 0.017483452335000038\n",
            "Batch loss: 0.07510746270418167\n",
            "Batch loss: 0.08750312775373459\n",
            "Batch loss: 0.046606700867414474\n",
            "Batch loss: 0.10522539913654327\n",
            "Batch loss: 0.05995930731296539\n",
            "Batch loss: 0.011473190039396286\n",
            "Batch loss: 0.04782388359308243\n",
            "Batch loss: 0.03109022043645382\n",
            "Batch loss: 0.022380243986845016\n",
            "Batch loss: 0.04185778275132179\n",
            "Batch loss: 0.02127959579229355\n",
            "Batch loss: 0.003487486159428954\n",
            "Batch loss: 0.0017111793858930469\n",
            "Batch loss: 0.0028989557176828384\n",
            "Batch loss: 0.03298959508538246\n",
            "Batch loss: 0.06545504927635193\n",
            "Batch loss: 0.0036957955453544855\n",
            "Batch loss: 0.0484536774456501\n",
            "Batch loss: 0.04112481325864792\n",
            "Batch loss: 0.00978492759168148\n",
            "Batch loss: 0.04062062129378319\n",
            "Batch loss: 0.02169625274837017\n",
            "Batch loss: 0.05935923382639885\n",
            "Batch loss: 0.03401828929781914\n",
            "Batch loss: 0.039240315556526184\n",
            "Batch loss: 0.03776723891496658\n",
            "Batch loss: 0.011134476400911808\n",
            "Batch loss: 0.04575743526220322\n",
            "Batch loss: 0.008482463657855988\n",
            "Batch loss: 0.0363469235599041\n",
            "Batch loss: 0.060258086770772934\n",
            "Batch loss: 0.0010407360969111323\n",
            "Batch loss: 0.011381188407540321\n",
            "Batch loss: 0.03830431029200554\n",
            "Batch loss: 0.008400024846196175\n",
            "Batch loss: 0.09606437385082245\n",
            "Batch loss: 0.1443324089050293\n",
            "Batch loss: 0.01611601561307907\n",
            "Batch loss: 0.0019003665074706078\n",
            "Batch loss: 0.004514812491834164\n",
            "Batch loss: 0.003932530991733074\n",
            "Batch loss: 0.13608375191688538\n",
            "Batch loss: 0.03412384167313576\n",
            "Batch loss: 0.005703556817024946\n",
            "Batch loss: 0.007307404652237892\n",
            "Batch loss: 0.06288722157478333\n",
            "Batch loss: 0.12123182415962219\n",
            "Batch loss: 0.03203989565372467\n",
            "Batch loss: 0.009883780963718891\n",
            "Batch loss: 0.006893550977110863\n",
            "Batch loss: 0.010210851207375526\n",
            "Batch loss: 0.01733323372900486\n",
            "Batch loss: 0.03066481463611126\n",
            "Batch loss: 0.011610410176217556\n",
            "Batch loss: 0.15765784680843353\n",
            "Batch loss: 0.004428068175911903\n",
            "Batch loss: 0.009163979440927505\n",
            "Batch loss: 0.007775990292429924\n",
            "Batch loss: 0.006603911519050598\n",
            "Batch loss: 0.025164185091853142\n",
            "Batch loss: 0.017970046028494835\n",
            "Batch loss: 0.0015387542080134153\n",
            "Batch loss: 0.0024334974586963654\n",
            "Batch loss: 0.011496766470372677\n",
            "Batch loss: 0.04702558368444443\n",
            "Batch loss: 0.09806885570287704\n",
            "Batch loss: 0.028987416997551918\n",
            "Batch loss: 0.010355370119214058\n",
            "Batch loss: 0.041741035878658295\n",
            "Batch loss: 0.012224787846207619\n",
            "Batch loss: 0.07272737473249435\n",
            "Batch loss: 0.010206938721239567\n",
            "Batch loss: 0.02599533274769783\n",
            "Batch loss: 0.14572666585445404\n",
            "Batch loss: 0.057747405022382736\n",
            "Batch loss: 0.08438419550657272\n",
            "Batch loss: 0.08617019653320312\n",
            "Batch loss: 0.0015517598949372768\n",
            "Batch loss: 0.1522645503282547\n",
            "Batch loss: 0.13628731667995453\n",
            "Batch loss: 0.056832846254110336\n",
            "Batch loss: 0.003219400532543659\n",
            "Batch loss: 0.02599390596151352\n",
            "Batch loss: 0.02211015857756138\n",
            "Batch loss: 0.016685206443071365\n",
            "Batch loss: 0.10877110064029694\n",
            "Batch loss: 0.11112843453884125\n",
            "Batch loss: 0.05675387382507324\n",
            "Batch loss: 0.03703447803854942\n",
            "Batch loss: 0.11295634508132935\n",
            "Batch loss: 0.023573141545057297\n",
            "Batch loss: 0.05030861869454384\n",
            "Batch loss: 0.023201465606689453\n",
            "Batch loss: 0.07066597044467926\n",
            "Batch loss: 0.03438510373234749\n",
            "Batch loss: 0.018235526978969574\n",
            "Batch loss: 0.010105227120220661\n",
            "Batch loss: 0.1338336318731308\n",
            "Batch loss: 0.05038372799754143\n",
            "Batch loss: 0.03288378566503525\n",
            "Batch loss: 0.02077186107635498\n",
            "Batch loss: 0.028804579749703407\n",
            "Batch loss: 0.03160732239484787\n",
            "Batch loss: 0.02773408778011799\n",
            "Batch loss: 0.09285178780555725\n",
            "Batch loss: 0.021469684317708015\n",
            "Batch loss: 0.1291743516921997\n",
            "Batch loss: 0.033266957849264145\n",
            "Batch loss: 0.06889338046312332\n",
            "Batch loss: 0.02153605967760086\n",
            "Batch loss: 0.029448052868247032\n",
            "Batch loss: 0.03615139052271843\n",
            "Batch loss: 0.024062078446149826\n",
            "Batch loss: 0.04176243767142296\n",
            "Batch loss: 0.03527474403381348\n",
            "Batch loss: 0.06325643509626389\n",
            "Batch loss: 0.017849862575531006\n",
            "Batch loss: 0.006078768987208605\n",
            "Batch loss: 0.04324367642402649\n",
            "Batch loss: 0.022332610562443733\n",
            "Batch loss: 0.03848244994878769\n",
            "Batch loss: 0.0678117498755455\n",
            "Batch loss: 0.05822759121656418\n",
            "Batch loss: 0.14714662730693817\n",
            "Batch loss: 0.016052236780524254\n",
            "Batch loss: 0.06322449445724487\n",
            "Batch loss: 0.01998450607061386\n",
            "Batch loss: 0.015148363076150417\n",
            "Batch loss: 0.016298802569508553\n",
            "Batch loss: 0.01602456346154213\n",
            "Batch loss: 0.03233054280281067\n",
            "Batch loss: 0.05825764313340187\n",
            "Batch loss: 0.044302549213171005\n",
            "Batch loss: 0.08038929849863052\n",
            "Batch loss: 0.13216927647590637\n",
            "Batch loss: 0.05811097100377083\n",
            "Batch loss: 0.07335350662469864\n",
            "Batch loss: 0.007623910438269377\n",
            "Batch loss: 0.12673182785511017\n",
            "Batch loss: 0.015500897541642189\n",
            "Batch loss: 0.028946930542588234\n",
            "Batch loss: 0.005597928073257208\n",
            "Batch loss: 0.044788654893636703\n",
            "Batch loss: 0.02589755319058895\n",
            "Batch loss: 0.0313020721077919\n",
            "Batch loss: 0.06404780596494675\n",
            "Batch loss: 0.009181791916489601\n",
            "Batch loss: 0.007826116867363453\n",
            "Batch loss: 0.16705743968486786\n",
            "Batch loss: 0.014153191819787025\n",
            "Batch loss: 0.06006145477294922\n",
            "Batch loss: 0.11026972532272339\n",
            "Batch loss: 0.02095342054963112\n",
            "Batch loss: 0.018165064975619316\n",
            "Batch loss: 0.04386608302593231\n",
            "Batch loss: 0.026026621460914612\n",
            "Batch loss: 0.07037604600191116\n",
            "Batch loss: 0.00777368014678359\n",
            "Batch loss: 0.005842149723321199\n",
            "Batch loss: 0.06043366342782974\n",
            "Batch loss: 0.014064943417906761\n",
            "Batch loss: 0.011786635965108871\n",
            "Batch loss: 0.04727349802851677\n",
            "Batch loss: 0.021596359089016914\n",
            "Batch loss: 0.04284035414457321\n",
            "Batch loss: 0.0781884714961052\n",
            "Batch loss: 0.01791030541062355\n",
            "Batch loss: 0.03813979774713516\n",
            "Batch loss: 0.05056154727935791\n",
            "Batch loss: 0.12214537709951401\n",
            "Batch loss: 0.0063930233009159565\n",
            "Batch loss: 0.08115196973085403\n",
            "Batch loss: 0.017273904755711555\n",
            "Batch loss: 0.0388060063123703\n",
            "Batch loss: 0.12282869964838028\n",
            "Batch loss: 0.00645822286605835\n",
            "Batch loss: 0.005666100420057774\n",
            "Batch loss: 0.015936631709337234\n",
            "Batch loss: 0.02814299240708351\n",
            "Batch loss: 0.013914906419813633\n",
            "Batch loss: 0.00990635808557272\n",
            "Batch loss: 0.24516989290714264\n",
            "Batch loss: 0.02147509716451168\n",
            "Batch loss: 0.048476722091436386\n",
            "Batch loss: 0.02230541966855526\n",
            "Batch loss: 0.09293977916240692\n",
            "Batch loss: 0.13030987977981567\n",
            "Batch loss: 0.07184016704559326\n",
            "Batch loss: 0.012756259180605412\n",
            "Batch loss: 0.011373347602784634\n",
            "Batch loss: 0.00624516187235713\n",
            "Batch loss: 0.003816857235506177\n",
            "Batch loss: 0.018435398116707802\n",
            "Batch loss: 0.014971120283007622\n",
            "Batch loss: 0.02172199636697769\n",
            "Batch loss: 0.01128456648439169\n",
            "Batch loss: 0.0445781908929348\n",
            "Batch loss: 0.015534736216068268\n",
            "Batch loss: 0.04491748660802841\n",
            "Batch loss: 0.0355038084089756\n",
            "Batch loss: 0.015601920895278454\n",
            "Batch loss: 0.03166687488555908\n",
            "Batch loss: 0.02896416373550892\n",
            "Batch loss: 0.029225768521428108\n",
            "Batch loss: 0.05564212054014206\n",
            "Batch loss: 0.03806367143988609\n",
            "Batch loss: 0.031378213316202164\n",
            "Batch loss: 0.02864469587802887\n",
            "Batch loss: 0.11707310378551483\n",
            "Batch loss: 0.03978857398033142\n",
            "Batch loss: 0.048265330493450165\n",
            "Batch loss: 0.06057174876332283\n",
            "Batch loss: 0.04378213360905647\n",
            "Batch loss: 0.011328822933137417\n",
            "Batch loss: 0.002150947228074074\n",
            "Batch loss: 0.045300789177417755\n",
            "Batch loss: 0.06942871958017349\n",
            "Batch loss: 0.023572631180286407\n",
            "Batch loss: 0.055103596299886703\n",
            "Batch loss: 0.05920761823654175\n",
            "Batch loss: 0.10618681460618973\n",
            "Batch loss: 0.12647902965545654\n",
            "Batch loss: 0.06390555202960968\n",
            "Batch loss: 0.11193345487117767\n",
            "Batch loss: 0.002065772656351328\n",
            "Batch loss: 0.01367238350212574\n",
            "Batch loss: 0.05893338471651077\n",
            "Batch loss: 0.0259008277207613\n",
            "Batch loss: 0.09743709117174149\n",
            "Batch loss: 0.01907733641564846\n",
            "Batch loss: 0.03789651766419411\n",
            "Batch loss: 0.05536281690001488\n",
            "Batch loss: 0.047845810651779175\n",
            "Batch loss: 0.026233749464154243\n",
            "Batch loss: 0.052730411291122437\n",
            "Batch loss: 0.0065504382364451885\n",
            "Batch loss: 0.03785637393593788\n",
            "Batch loss: 0.04111532121896744\n",
            "Batch loss: 0.005156928673386574\n",
            "Batch loss: 0.07382184267044067\n",
            "Batch loss: 0.0326366052031517\n",
            "Batch loss: 0.07575219869613647\n",
            "Batch loss: 0.009906223975121975\n",
            "Batch loss: 0.007807168178260326\n",
            "Batch loss: 0.015010939911007881\n",
            "Batch loss: 0.006669547408819199\n",
            "Batch loss: 0.0276330579072237\n",
            "Batch loss: 0.012737954035401344\n",
            "Batch loss: 0.04814939945936203\n",
            "Batch loss: 0.009368127211928368\n",
            "Batch loss: 0.010449486784636974\n",
            "Batch loss: 0.014975164085626602\n",
            "Batch loss: 0.03080589324235916\n",
            "Batch loss: 0.009663298726081848\n",
            "Batch loss: 0.0426124706864357\n",
            "Batch loss: 0.06676964461803436\n",
            "Batch loss: 0.061806727200746536\n",
            "Batch loss: 0.042344119399785995\n",
            "Batch loss: 0.0019253691425547004\n",
            "Batch loss: 0.003731386736035347\n",
            "Batch loss: 0.009788110852241516\n",
            "Batch loss: 0.0009873510571196675\n",
            "Batch loss: 0.08125404268503189\n",
            "Batch loss: 0.004045926965773106\n",
            "Batch loss: 0.07556751370429993\n",
            "Batch loss: 0.013468741439282894\n",
            "Batch loss: 0.0024915775284171104\n",
            "Batch loss: 0.011184067465364933\n",
            "Batch loss: 0.01259832177311182\n",
            "Batch loss: 0.05633919686079025\n",
            "Batch loss: 0.06659172475337982\n",
            "Batch loss: 0.03492937609553337\n",
            "Batch loss: 0.023377496749162674\n",
            "Batch loss: 0.004251062870025635\n",
            "Batch loss: 0.006361035164445639\n",
            "Batch loss: 0.02500137872993946\n",
            "Batch loss: 0.007509012706577778\n",
            "Batch loss: 0.013488834723830223\n",
            "Batch loss: 0.004020438063889742\n",
            "Batch loss: 0.06782519072294235\n",
            "End of epoch\n",
            "Start of epoch\n",
            "Batch loss: 0.00764860026538372\n",
            "Batch loss: 0.005716484971344471\n",
            "Batch loss: 0.11065854132175446\n",
            "Batch loss: 0.003407088341191411\n",
            "Batch loss: 0.1793270856142044\n",
            "Batch loss: 0.03798719495534897\n",
            "Batch loss: 0.004024403635412455\n",
            "Batch loss: 0.004156285896897316\n",
            "Batch loss: 0.054249923676252365\n",
            "Batch loss: 0.004050340969115496\n",
            "Batch loss: 0.026198236271739006\n",
            "Batch loss: 0.006989773362874985\n",
            "Batch loss: 0.029135724529623985\n",
            "Batch loss: 0.003907682839781046\n",
            "Batch loss: 0.0038991516921669245\n",
            "Batch loss: 0.01294529065489769\n",
            "Batch loss: 0.006911843083798885\n",
            "Batch loss: 0.01715051382780075\n",
            "Batch loss: 0.00838859286159277\n",
            "Batch loss: 0.01193591021001339\n",
            "Batch loss: 0.014720283448696136\n",
            "Batch loss: 0.004323293920606375\n",
            "Batch loss: 0.022837786003947258\n",
            "Batch loss: 0.012881917878985405\n",
            "Batch loss: 0.06649236381053925\n",
            "Batch loss: 0.06395003199577332\n",
            "Batch loss: 0.032122187316417694\n",
            "Batch loss: 0.01348407194018364\n",
            "Batch loss: 0.010784227401018143\n",
            "Batch loss: 0.011111786589026451\n",
            "Batch loss: 0.019810479134321213\n",
            "Batch loss: 0.034130167216062546\n",
            "Batch loss: 0.01535312831401825\n",
            "Batch loss: 0.011090204119682312\n",
            "Batch loss: 0.0056826043874025345\n",
            "Batch loss: 0.002383327344432473\n",
            "Batch loss: 0.08866628259420395\n",
            "Batch loss: 0.0030924140010029078\n",
            "Batch loss: 0.14061595499515533\n",
            "Batch loss: 0.0502774752676487\n",
            "Batch loss: 0.0029251219239085913\n",
            "Batch loss: 0.011303273029625416\n",
            "Batch loss: 0.05018265172839165\n",
            "Batch loss: 0.008480471558868885\n",
            "Batch loss: 0.06470747292041779\n",
            "Batch loss: 0.01647164858877659\n",
            "Batch loss: 0.15855887532234192\n",
            "Batch loss: 0.004714413080364466\n",
            "Batch loss: 0.006385031156241894\n",
            "Batch loss: 0.00978086981922388\n",
            "Batch loss: 0.011930830776691437\n",
            "Batch loss: 0.0036932521034032106\n",
            "Batch loss: 0.004919933155179024\n",
            "Batch loss: 0.07753448933362961\n",
            "Batch loss: 0.005219899117946625\n",
            "Batch loss: 0.02619645930826664\n",
            "Batch loss: 0.008059506304562092\n",
            "Batch loss: 0.05915747210383415\n",
            "Batch loss: 0.0015844993758946657\n",
            "Batch loss: 0.005024149548262358\n",
            "Batch loss: 0.021412748843431473\n",
            "Batch loss: 0.02574562095105648\n",
            "Batch loss: 0.0032218799460679293\n",
            "Batch loss: 0.018269188702106476\n",
            "Batch loss: 0.0040525998920202255\n",
            "Batch loss: 0.015042503364384174\n",
            "Batch loss: 0.003949410747736692\n",
            "Batch loss: 0.008864152245223522\n",
            "Batch loss: 0.006475264672189951\n",
            "Batch loss: 0.011728588491678238\n",
            "Batch loss: 0.029199950397014618\n",
            "Batch loss: 0.02822710946202278\n",
            "Batch loss: 0.0032511551398783922\n",
            "Batch loss: 0.003442484186962247\n",
            "Batch loss: 0.0301885437220335\n",
            "Batch loss: 0.07218188047409058\n",
            "Batch loss: 0.00964866578578949\n",
            "Batch loss: 0.013996852561831474\n",
            "Batch loss: 0.010182210244238377\n",
            "Batch loss: 0.00426558218896389\n",
            "Batch loss: 0.035503119230270386\n",
            "Batch loss: 0.06458839029073715\n",
            "Batch loss: 0.0022059588227421045\n",
            "Batch loss: 0.08310840278863907\n",
            "Batch loss: 0.0178024061024189\n",
            "Batch loss: 0.007759678177535534\n",
            "Batch loss: 0.04176388680934906\n",
            "Batch loss: 0.003567869309335947\n",
            "Batch loss: 0.003096597036346793\n",
            "Batch loss: 0.004279745742678642\n",
            "Batch loss: 0.02732216566801071\n",
            "Batch loss: 0.03096073493361473\n",
            "Batch loss: 0.05726703256368637\n",
            "Batch loss: 0.016408681869506836\n",
            "Batch loss: 0.0029022942762821913\n",
            "Batch loss: 0.03758092224597931\n",
            "Batch loss: 0.0037852488458156586\n",
            "Batch loss: 0.00143247761297971\n",
            "Batch loss: 0.02497927099466324\n",
            "Batch loss: 0.0011839652433991432\n",
            "Batch loss: 0.0024219551123678684\n",
            "Batch loss: 0.008111481554806232\n",
            "Batch loss: 0.005531062837690115\n",
            "Batch loss: 0.004504510201513767\n",
            "Batch loss: 0.03300175815820694\n",
            "Batch loss: 0.019226685166358948\n",
            "Batch loss: 0.005913008935749531\n",
            "Batch loss: 0.034731175750494\n",
            "Batch loss: 0.0007443105569109321\n",
            "Batch loss: 0.04035292565822601\n",
            "Batch loss: 0.03726468235254288\n",
            "Batch loss: 0.010691453702747822\n",
            "Batch loss: 0.0034429465886205435\n",
            "Batch loss: 0.06520391255617142\n",
            "Batch loss: 0.0014252158580347896\n",
            "Batch loss: 0.005086387507617474\n",
            "Batch loss: 0.009547559544444084\n",
            "Batch loss: 0.009098450653254986\n",
            "Batch loss: 0.006813821382820606\n",
            "Batch loss: 0.004296490456908941\n",
            "Batch loss: 0.05790804699063301\n",
            "Batch loss: 0.011723414994776249\n",
            "Batch loss: 0.0012657474726438522\n",
            "Batch loss: 0.0018894815584644675\n",
            "Batch loss: 0.00794548075646162\n",
            "Batch loss: 0.001729689771309495\n",
            "Batch loss: 0.030746763572096825\n",
            "Batch loss: 0.007365546654909849\n",
            "Batch loss: 0.030226245522499084\n",
            "Batch loss: 0.0175047405064106\n",
            "Batch loss: 0.020411280915141106\n",
            "Batch loss: 0.0045670210383832455\n",
            "Batch loss: 0.025127045810222626\n",
            "Batch loss: 0.0016328280325978994\n",
            "Batch loss: 0.00204843538813293\n",
            "Batch loss: 0.023977363482117653\n",
            "Batch loss: 0.002237292006611824\n",
            "Batch loss: 0.008146366104483604\n",
            "Batch loss: 0.0007278349949046969\n",
            "Batch loss: 0.0010647464077919722\n",
            "Batch loss: 0.020871959626674652\n",
            "Batch loss: 0.019196411594748497\n",
            "Batch loss: 0.03708706423640251\n",
            "Batch loss: 0.03183818235993385\n",
            "Batch loss: 0.039997294545173645\n",
            "Batch loss: 0.006752324290573597\n",
            "Batch loss: 0.0011851434828713536\n",
            "Batch loss: 0.16801400482654572\n",
            "Batch loss: 0.006133762653917074\n",
            "Batch loss: 0.009216107428073883\n",
            "Batch loss: 0.05579737201333046\n",
            "Batch loss: 0.0071383630856871605\n",
            "Batch loss: 0.055635757744312286\n",
            "Batch loss: 0.03748603165149689\n",
            "Batch loss: 0.0038873679004609585\n",
            "Batch loss: 0.00539954099804163\n",
            "Batch loss: 0.09611745923757553\n",
            "Batch loss: 0.021563196554780006\n",
            "Batch loss: 0.015287213958799839\n",
            "Batch loss: 0.01672949828207493\n",
            "Batch loss: 0.015363510698080063\n",
            "Batch loss: 0.00567253865301609\n",
            "Batch loss: 0.0017173875821754336\n",
            "Batch loss: 0.007189091760665178\n",
            "Batch loss: 0.006564901676028967\n",
            "Batch loss: 0.03776418790221214\n",
            "Batch loss: 0.006586414761841297\n",
            "Batch loss: 0.015566346235573292\n",
            "Batch loss: 0.002772087464109063\n",
            "Batch loss: 0.0028600157238543034\n",
            "Batch loss: 0.048023369163274765\n",
            "Batch loss: 0.0036355401389300823\n",
            "Batch loss: 0.032585419714450836\n",
            "Batch loss: 0.008551682345569134\n",
            "Batch loss: 0.00411033071577549\n",
            "Batch loss: 0.011222206987440586\n",
            "Batch loss: 0.005136944353580475\n",
            "Batch loss: 0.03513745963573456\n",
            "Batch loss: 0.057871054857969284\n",
            "Batch loss: 0.005621687043458223\n",
            "Batch loss: 0.004412566777318716\n",
            "Batch loss: 0.03649485856294632\n",
            "Batch loss: 0.0062888143584132195\n",
            "Batch loss: 0.04217808321118355\n",
            "Batch loss: 0.014371447265148163\n",
            "Batch loss: 0.0028777059633284807\n",
            "Batch loss: 0.008269562385976315\n",
            "Batch loss: 0.0027169513050466776\n",
            "Batch loss: 0.046762678772211075\n",
            "Batch loss: 0.0035025146789848804\n",
            "Batch loss: 0.025210333988070488\n",
            "Batch loss: 0.008670407347381115\n",
            "Batch loss: 0.020821403712034225\n",
            "Batch loss: 0.005644265096634626\n",
            "Batch loss: 0.02845517359673977\n",
            "Batch loss: 0.0028568929992616177\n",
            "Batch loss: 0.00499078119173646\n",
            "Batch loss: 0.012459328398108482\n",
            "Batch loss: 0.0068705384619534016\n",
            "Batch loss: 0.0007665641023777425\n",
            "Batch loss: 0.1015097051858902\n",
            "Batch loss: 0.03614184260368347\n",
            "Batch loss: 0.004260609392076731\n",
            "Batch loss: 0.016805453225970268\n",
            "Batch loss: 0.010942615568637848\n",
            "Batch loss: 0.00952767301350832\n",
            "Batch loss: 0.006911624688655138\n",
            "Batch loss: 0.001268891035579145\n",
            "Batch loss: 0.143373504281044\n",
            "Batch loss: 0.0037524327635765076\n",
            "Batch loss: 0.0016706236638128757\n",
            "Batch loss: 0.016847720369696617\n",
            "Batch loss: 0.01857714168727398\n",
            "Batch loss: 0.01544139999896288\n",
            "Batch loss: 0.0036943722516298294\n",
            "Batch loss: 0.024024898186326027\n",
            "Batch loss: 0.012259727343916893\n",
            "Batch loss: 0.004910244606435299\n",
            "Batch loss: 0.003794181626290083\n",
            "Batch loss: 0.00629793293774128\n",
            "Batch loss: 0.02265015058219433\n",
            "Batch loss: 0.006293711252510548\n",
            "Batch loss: 0.0034952766727656126\n",
            "Batch loss: 0.024992533028125763\n",
            "Batch loss: 0.0017279816092923284\n",
            "Batch loss: 0.002420532051473856\n",
            "Batch loss: 0.0062943268567323685\n",
            "Batch loss: 0.0025921077467501163\n",
            "Batch loss: 0.0004396431031636894\n",
            "Batch loss: 0.1149115338921547\n",
            "Batch loss: 0.016050586476922035\n",
            "Batch loss: 0.014286234974861145\n",
            "Batch loss: 0.006227295380085707\n",
            "Batch loss: 0.0029452869202941656\n",
            "Batch loss: 0.015355449169874191\n",
            "Batch loss: 0.004722727928310633\n",
            "Batch loss: 0.01082724891602993\n",
            "Batch loss: 0.019400356337428093\n",
            "Batch loss: 0.013090120628476143\n",
            "Batch loss: 0.027419215068221092\n",
            "Batch loss: 0.0007810159586369991\n",
            "Batch loss: 0.0015782712725922465\n",
            "Batch loss: 0.017020223662257195\n",
            "Batch loss: 0.0016841132892295718\n",
            "Batch loss: 0.007431766949594021\n",
            "Batch loss: 0.0182860866189003\n",
            "Batch loss: 0.002969995839521289\n",
            "Batch loss: 0.08276291936635971\n",
            "Batch loss: 0.0043993364088237286\n",
            "Batch loss: 0.019759804010391235\n",
            "Batch loss: 0.007330338470637798\n",
            "Batch loss: 0.04325278475880623\n",
            "Batch loss: 0.006533964537084103\n",
            "Batch loss: 0.0014729464892297983\n",
            "Batch loss: 0.002776728942990303\n",
            "Batch loss: 0.025100797414779663\n",
            "Batch loss: 0.026107842102646828\n",
            "Batch loss: 0.0023285860661417246\n",
            "Batch loss: 0.009712508879601955\n",
            "Batch loss: 0.006345039699226618\n",
            "Batch loss: 0.001516867894679308\n",
            "Batch loss: 0.018599320203065872\n",
            "Batch loss: 0.03607868030667305\n",
            "Batch loss: 0.0019096758915111423\n",
            "Batch loss: 0.010145802050828934\n",
            "Batch loss: 0.00019687629537656903\n",
            "Batch loss: 0.0011593312956392765\n",
            "Batch loss: 0.010957983322441578\n",
            "Batch loss: 0.016683002933859825\n",
            "Batch loss: 0.07405947893857956\n",
            "Batch loss: 0.008060520514845848\n",
            "Batch loss: 0.00311465747654438\n",
            "Batch loss: 0.007349471561610699\n",
            "Batch loss: 0.0028492033015936613\n",
            "Batch loss: 0.0016129011055454612\n",
            "Batch loss: 0.07899458706378937\n",
            "Batch loss: 0.007497467566281557\n",
            "Batch loss: 0.009631453081965446\n",
            "Batch loss: 0.023922504857182503\n",
            "Batch loss: 0.02223370596766472\n",
            "Batch loss: 0.08209923654794693\n",
            "Batch loss: 0.051269493997097015\n",
            "Batch loss: 0.012609083205461502\n",
            "Batch loss: 0.014693645760416985\n",
            "Batch loss: 0.004173721186816692\n",
            "Batch loss: 0.0716957300901413\n",
            "Batch loss: 0.006768608931452036\n",
            "Batch loss: 0.0043507833033800125\n",
            "Batch loss: 0.006274575833231211\n",
            "Batch loss: 0.005457248073071241\n",
            "Batch loss: 0.008946876972913742\n",
            "Batch loss: 0.002577075269073248\n",
            "Batch loss: 0.014111124910414219\n",
            "Batch loss: 0.02915394864976406\n",
            "Batch loss: 0.01256174873560667\n",
            "Batch loss: 0.02465389482676983\n",
            "Batch loss: 0.04152226820588112\n",
            "Batch loss: 0.003879337105900049\n",
            "Batch loss: 0.0013913663569837809\n",
            "Batch loss: 0.12861070036888123\n",
            "Batch loss: 0.044788096100091934\n",
            "Batch loss: 0.006682034581899643\n",
            "Batch loss: 0.0034845625050365925\n",
            "Batch loss: 0.022005507722496986\n",
            "Batch loss: 0.04936568811535835\n",
            "Batch loss: 0.009321387857198715\n",
            "Batch loss: 0.007182027213275433\n",
            "Batch loss: 0.026638532057404518\n",
            "Batch loss: 0.0007827355875633657\n",
            "Batch loss: 0.0015813298523426056\n",
            "Batch loss: 0.002028790535405278\n",
            "Batch loss: 0.008537737652659416\n",
            "Batch loss: 0.006899700500071049\n",
            "Batch loss: 0.001677128137089312\n",
            "Batch loss: 0.00498733576387167\n",
            "Batch loss: 0.019403373822569847\n",
            "Batch loss: 0.012597808614373207\n",
            "Batch loss: 0.0076893651857972145\n",
            "Batch loss: 0.002625014167279005\n",
            "Batch loss: 0.003982674330472946\n",
            "Batch loss: 0.0060048094019293785\n",
            "Batch loss: 0.0021428484469652176\n",
            "Batch loss: 0.01529501099139452\n",
            "Batch loss: 0.0027328552678227425\n",
            "Batch loss: 0.016090234741568565\n",
            "Batch loss: 0.0156560018658638\n",
            "Batch loss: 0.019196677953004837\n",
            "Batch loss: 0.01246481854468584\n",
            "Batch loss: 0.001720699481666088\n",
            "Batch loss: 0.003463180037215352\n",
            "Batch loss: 0.03807641565799713\n",
            "Batch loss: 0.01342364028096199\n",
            "Batch loss: 0.00428580678999424\n",
            "Batch loss: 0.0014522435376420617\n",
            "Batch loss: 0.00219172821380198\n",
            "Batch loss: 0.009430363774299622\n",
            "Batch loss: 0.021293168887495995\n",
            "Batch loss: 0.015319685451686382\n",
            "Batch loss: 0.028996258974075317\n",
            "Batch loss: 0.05355248227715492\n",
            "Batch loss: 0.004013856872916222\n",
            "Batch loss: 0.012402343563735485\n",
            "Batch loss: 0.009362712502479553\n",
            "Batch loss: 0.007267525419592857\n",
            "Batch loss: 0.06393848359584808\n",
            "Batch loss: 0.10009240359067917\n",
            "Batch loss: 0.02388942800462246\n",
            "Batch loss: 0.022782674059271812\n",
            "Batch loss: 0.014367178082466125\n",
            "Batch loss: 0.003666254924610257\n",
            "Batch loss: 0.05617635324597359\n",
            "Batch loss: 0.01161949709057808\n",
            "Batch loss: 0.0071892584674060345\n",
            "Batch loss: 0.05601992458105087\n",
            "Batch loss: 0.02732146345078945\n",
            "Batch loss: 0.014538226649165154\n",
            "Batch loss: 0.0015451073413714767\n",
            "Batch loss: 0.002505242358893156\n",
            "Batch loss: 0.017329301685094833\n",
            "Batch loss: 0.019241366535425186\n",
            "Batch loss: 0.034633662551641464\n",
            "Batch loss: 0.032291002571582794\n",
            "Batch loss: 0.05026329308748245\n",
            "Batch loss: 0.003267786232754588\n",
            "Batch loss: 0.02973436377942562\n",
            "Batch loss: 0.006837064400315285\n",
            "Batch loss: 0.003988574258983135\n",
            "Batch loss: 0.002026309259235859\n",
            "Batch loss: 0.0011119627160951495\n",
            "Batch loss: 0.0007409043610095978\n",
            "Batch loss: 0.0009981926996260881\n",
            "Batch loss: 0.08646581321954727\n",
            "Batch loss: 0.002826172159984708\n",
            "Batch loss: 0.034391697496175766\n",
            "Batch loss: 0.03263700008392334\n",
            "Batch loss: 0.06521634757518768\n",
            "Batch loss: 0.0007849811809137464\n",
            "Batch loss: 0.08380899578332901\n",
            "Batch loss: 0.022424861788749695\n",
            "Batch loss: 0.006480915006250143\n",
            "Batch loss: 0.015285279601812363\n",
            "Batch loss: 0.12871868908405304\n",
            "Batch loss: 0.03306935355067253\n",
            "Batch loss: 0.020034577697515488\n",
            "Batch loss: 0.00626703817397356\n",
            "Batch loss: 0.019540635868906975\n",
            "Batch loss: 0.0007624643039889634\n",
            "Batch loss: 0.03424922376871109\n",
            "Batch loss: 0.053075894713401794\n",
            "Batch loss: 0.02502831071615219\n",
            "Batch loss: 0.00450239609926939\n",
            "Batch loss: 0.07705599069595337\n",
            "Batch loss: 0.021811526268720627\n",
            "Batch loss: 0.009247958660125732\n",
            "Batch loss: 0.03807586058974266\n",
            "Batch loss: 0.009137764573097229\n",
            "Batch loss: 0.029606711119413376\n",
            "Batch loss: 0.013575210236012936\n",
            "Batch loss: 0.00851611327379942\n",
            "Batch loss: 0.002066640881821513\n",
            "Batch loss: 0.009859933517873287\n",
            "Batch loss: 0.019630394876003265\n",
            "Batch loss: 0.021722955629229546\n",
            "Batch loss: 0.06033342331647873\n",
            "Batch loss: 0.017465196549892426\n",
            "Batch loss: 0.012810052372515202\n",
            "Batch loss: 0.01042721327394247\n",
            "Batch loss: 0.0026662065647542477\n",
            "Batch loss: 0.016465814784169197\n",
            "Batch loss: 0.12995171546936035\n",
            "Batch loss: 0.013923989608883858\n",
            "Batch loss: 0.04145526885986328\n",
            "Batch loss: 0.010134712792932987\n",
            "Batch loss: 0.022581741213798523\n",
            "Batch loss: 0.008565342985093594\n",
            "Batch loss: 0.07222770899534225\n",
            "Batch loss: 0.02261025831103325\n",
            "Batch loss: 0.005988840013742447\n",
            "Batch loss: 0.007847434841096401\n",
            "Batch loss: 0.0069951871410012245\n",
            "Batch loss: 0.012898186221718788\n",
            "Batch loss: 0.018618425354361534\n",
            "Batch loss: 0.01796220801770687\n",
            "Batch loss: 0.015379500575363636\n",
            "Batch loss: 0.018064366653561592\n",
            "Batch loss: 0.032421793788671494\n",
            "Batch loss: 0.00493959104642272\n",
            "Batch loss: 0.030711691826581955\n",
            "Batch loss: 0.004739240277558565\n",
            "Batch loss: 0.07259280234575272\n",
            "Batch loss: 0.007665968034416437\n",
            "Batch loss: 0.031822845339775085\n",
            "Batch loss: 0.04515067860484123\n",
            "Batch loss: 0.027197007089853287\n",
            "Batch loss: 0.0019789261277765036\n",
            "Batch loss: 0.011914409697055817\n",
            "Batch loss: 0.014134757220745087\n",
            "Batch loss: 0.00503222830593586\n",
            "Batch loss: 0.043816372752189636\n",
            "Batch loss: 0.0028209721203893423\n",
            "Batch loss: 0.002302346285432577\n",
            "Batch loss: 0.002626762492582202\n",
            "Batch loss: 0.0504474863409996\n",
            "Batch loss: 0.06695244461297989\n",
            "Batch loss: 0.01719217374920845\n",
            "Batch loss: 0.0017649306682869792\n",
            "Batch loss: 0.10425719618797302\n",
            "Batch loss: 0.0023986876476556063\n",
            "Batch loss: 0.0021856955718249083\n",
            "Batch loss: 0.14085376262664795\n",
            "Batch loss: 0.052036408334970474\n",
            "Batch loss: 0.04939733445644379\n",
            "Batch loss: 0.007398730143904686\n",
            "Batch loss: 0.0033156718127429485\n",
            "Batch loss: 0.038417138159275055\n",
            "Batch loss: 0.05691320076584816\n",
            "Batch loss: 0.016557874158024788\n",
            "Batch loss: 0.0034788341727107763\n",
            "Batch loss: 0.07265165448188782\n",
            "Batch loss: 0.06207320839166641\n",
            "Batch loss: 0.027242423966526985\n",
            "Batch loss: 0.0098580876365304\n",
            "Batch loss: 0.012248829007148743\n",
            "Batch loss: 0.09459121525287628\n",
            "Batch loss: 0.04986923560500145\n",
            "Batch loss: 0.005738791543990374\n",
            "Batch loss: 0.10463257133960724\n",
            "Batch loss: 0.007589260116219521\n",
            "Batch loss: 0.03356644883751869\n",
            "Batch loss: 0.020512597635388374\n",
            "Batch loss: 0.010208165273070335\n",
            "Batch loss: 0.00850700493901968\n",
            "Batch loss: 0.04043400660157204\n",
            "Batch loss: 0.01639806479215622\n",
            "Batch loss: 0.003119213506579399\n",
            "Batch loss: 0.013804291374981403\n",
            "Batch loss: 0.03393353894352913\n",
            "Batch loss: 0.054455775767564774\n",
            "Batch loss: 0.025188326835632324\n",
            "Batch loss: 0.0033801121171563864\n",
            "Batch loss: 0.0049011241644620895\n",
            "Batch loss: 0.009979793801903725\n",
            "Batch loss: 0.03082966059446335\n",
            "Batch loss: 0.0029959003441035748\n",
            "Batch loss: 0.0033621436450630426\n",
            "Batch loss: 0.046045757830142975\n",
            "Batch loss: 0.0015108354855328798\n",
            "Batch loss: 0.02955758199095726\n",
            "Batch loss: 0.004096877761185169\n",
            "Batch loss: 0.067777119576931\n",
            "Batch loss: 0.058119095861911774\n",
            "Batch loss: 0.014443219639360905\n",
            "Batch loss: 0.02116553485393524\n",
            "Batch loss: 0.0029289789963513613\n",
            "Batch loss: 0.030126389116048813\n",
            "Batch loss: 0.00026861816877499223\n",
            "Batch loss: 0.008400073274970055\n",
            "Batch loss: 0.0013982370728626847\n",
            "Batch loss: 0.02080364152789116\n",
            "Batch loss: 0.08711174875497818\n",
            "Batch loss: 0.044150460511446\n",
            "Batch loss: 0.011063364334404469\n",
            "Batch loss: 0.09926458448171616\n",
            "Batch loss: 0.02169274352490902\n",
            "Batch loss: 0.03288969397544861\n",
            "Batch loss: 0.05684356018900871\n",
            "Batch loss: 0.005407411605119705\n",
            "Batch loss: 0.0604432187974453\n",
            "Batch loss: 0.0024815138895064592\n",
            "Batch loss: 0.006018972024321556\n",
            "Batch loss: 0.011564912274479866\n",
            "Batch loss: 0.10668281465768814\n",
            "Batch loss: 0.057739708572626114\n",
            "Batch loss: 0.053535345941782\n",
            "Batch loss: 0.008582957088947296\n",
            "Batch loss: 0.01957337185740471\n",
            "Batch loss: 0.007826787419617176\n",
            "Batch loss: 0.014877441339194775\n",
            "Batch loss: 0.021641751751303673\n",
            "Batch loss: 0.00367419864051044\n",
            "Batch loss: 0.0027436716482043266\n",
            "Batch loss: 0.0006818505935370922\n",
            "Batch loss: 0.0016196086071431637\n",
            "Batch loss: 0.028284909203648567\n",
            "Batch loss: 0.03477655351161957\n",
            "Batch loss: 0.009243653155863285\n",
            "Batch loss: 0.1889876127243042\n",
            "Batch loss: 0.02863241918385029\n",
            "Batch loss: 0.0027370855677872896\n",
            "Batch loss: 0.0889989510178566\n",
            "Batch loss: 0.014718165621161461\n",
            "Batch loss: 0.007378203794360161\n",
            "Batch loss: 0.012829906307160854\n",
            "Batch loss: 0.011252587661147118\n",
            "Batch loss: 0.11087147891521454\n",
            "Batch loss: 0.08201254159212112\n",
            "Batch loss: 0.007884930819272995\n",
            "Batch loss: 0.04036661237478256\n",
            "Batch loss: 0.0020427831914275885\n",
            "Batch loss: 0.0204326119273901\n",
            "Batch loss: 0.0065912543796002865\n",
            "Batch loss: 0.06708225607872009\n",
            "Batch loss: 0.009657712653279305\n",
            "Batch loss: 0.007862682454288006\n",
            "Batch loss: 0.020948586985468864\n",
            "Batch loss: 0.010741397738456726\n",
            "Batch loss: 0.020677294582128525\n",
            "End of epoch\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "if input(\"Are you sure you want to continue training and saving models? Select (Y/N): \") != \"Y\":\n",
        "    exit()\n",
        "else:\n",
        "  num_epochs = int(input(\"How many more epochs?: \"))\n",
        "  learning_rate = int(input(\"Learning rate: \"))\n",
        "  num_classes = 45\n",
        "  model = torchvision.models.resnet152(pretrained=True)\n",
        "  num_features = model.fc.in_features\n",
        "  model.fc = nn.Linear(num_features, num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  PATH = \"/content/drive/MyDrive/2022-04-26_model_10.pth\"\n",
        "\n",
        "  if device == 'cuda':\n",
        "    model.load_state_dict(torch.load(PATH, map_location=torch.device('cuda')))\n",
        "  else:\n",
        "    model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
        "  \n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "  model.train()\n",
        "  #inputs, labels = next(iter(train_loader))\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Start of epoch\")\n",
        "    for j, (inputs, labels) in enumerate(train_loader, start=0):\n",
        "\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs) #forwards\n",
        "      loss = criterion(outputs, labels) #calculate loss\n",
        "      optimizer.zero_grad() #zero the parameter gradients\n",
        "      loss.backward() #backwards\n",
        "      optimizer.step() #optimize\n",
        "\n",
        "      print(f'Batch loss: {loss.item()}')\n",
        "\n",
        "      #scheduler.step()\n",
        "    print(\"End of epoch\")\n",
        "  #https://stackoverflow.com/questions/32490629/getting-todays-date-in-yyyy-mm-dd-in-python\n",
        "  today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "  path = f'/content/drive/MyDrive/{today}_model_13.pth'\n",
        "  torch.save(model.state_dict(), path)\n",
        "  print('Model saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRVDI9myTQFN"
      },
      "source": [
        "Validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPtoDoiyUSQL",
        "outputId": "7ad4ce13-79f6-44b3-98bd-4ae4c3d18735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 accuracy is 95.11371312767395%\n"
          ]
        }
      ],
      "source": [
        "#importing pretrained model and changing output layer\n",
        "num_classes = 45\n",
        "model = torchvision.models.resnet152(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/2022-04-26_model_12.pth\"\n",
        "\n",
        "if device == 'cuda':\n",
        "  model.load_state_dict(torch.load(PATH, map_location=torch.device('cuda')))\n",
        "else:\n",
        "  model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "accuracy_denom = len(val_dataset)\n",
        "top1_accuracy_numer = 0\n",
        "with torch.no_grad():\n",
        "  for j, data in enumerate(val_loader, start=0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs,1)\n",
        "    #just like the labels, predicted is a tensor of (usually) batch_size length, with nums corresponding to the most likely classes\n",
        "    #top-1 accuracy\n",
        "    for k in range(len(predicted)):\n",
        "      top1_accuracy_numer += 1 if predicted[k] == labels[k] else 0\n",
        "print(f\"Top-1 accuracy is {top1_accuracy_numer*100/accuracy_denom}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLLL2V9lTTBp"
      },
      "source": [
        "Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l176LfKzTOny",
        "outputId": "6258bffb-d281-4cca-ecf6-d81a4f36c39b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 accuracy is 93.88%\n"
          ]
        }
      ],
      "source": [
        "#importing pretrained model and changing output layer\n",
        "num_classes = 45\n",
        "model = torchvision.models.resnet152(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/2022-04-26_model_8.pth\"\n",
        "\n",
        "if device == 'cuda':\n",
        "  model.load_state_dict(torch.load(PATH, map_location=torch.device('cuda')))\n",
        "else:\n",
        "  model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "accuracy_denom = len(test_dataset)\n",
        "top1_accuracy_numer = 0\n",
        "with torch.no_grad():\n",
        "  for j, data in enumerate(test_loader, start=0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs,1)\n",
        "    #just like the labels, predicted is a tensor of (usually) batch_size length, with nums corresponding to the most likely classes\n",
        "    #top-1 accuracy\n",
        "    for k in range(len(predicted)):\n",
        "      top1_accuracy_numer += 1 if predicted[k] == labels[k] else 0\n",
        "print(f\"Top-1 accuracy is {top1_accuracy_numer*100/accuracy_denom}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}